{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0bfa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.errors\n",
    "from pymongo import MongoClient\n",
    "from cassandra.cluster import Cluster\n",
    "import mysql.connector\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "with open('docker-compose.yaml', 'r') as file:\n",
    "    docker_config = yaml.safe_load(file)\n",
    "\n",
    "postgres_config = docker_config['services']['postgres']\n",
    "postgres_client = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    database=postgres_config['environment']['POSTGRES_DB'],\n",
    "    user=postgres_config['environment']['POSTG RES_USER'],\n",
    "    password=postgres_config['environment']['POSTGRES_PASSWORD'],\n",
    "    port=postgres_config['ports'][0].split(':')[0]\n",
    ")\n",
    "\n",
    "mariadb_config = docker_config['services']['mariadb']\n",
    "mariadb_client = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    database=mariadb_config['environment']['MYSQL_DATABASE'],\n",
    "    user=mariadb_config['environment']['MYSQL_USER'],\n",
    "    password=mariadb_config['environment']['MYSQL_PASSWORD'],\n",
    "    port=mariadb_config['ports'][0].split(':')[0],\n",
    "    allow_local_infile=True\n",
    ")\n",
    "\n",
    "mysql_config = docker_config['services']['mysql']\n",
    "mysql_client = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    database=mysql_config['environment']['MYSQL_DATABASE'],\n",
    "    user=mysql_config['environment']['MYSQL_USER'],\n",
    "    password=mysql_config['environment']['MYSQL_PASSWORD'],\n",
    "    port=mysql_config['ports'][0].split(':')[0],\n",
    "    allow_local_infile=True\n",
    ")\n",
    "\n",
    "mongo_8_config = docker_config['services']['mongo-8']\n",
    "mongo_8_config = MongoClient(\n",
    "    host='localhost',\n",
    "    port=int(mongo_8_config['ports'][0].split(':')[0])\n",
    ")\n",
    "\n",
    "mongo_7_config = docker_config['services']['mongo-7']\n",
    "mongo_7_config = MongoClient(\n",
    "    host='localhost',\n",
    "    port=int(mongo_7_config['ports'][0].split(':')[0])\n",
    ")\n",
    "\n",
    "cassandra_config = docker_config['services']['cassandra']\n",
    "cassandra_client = Cluster(['localhost'], port=cassandra_config['ports'][0].split(':')[0])\n",
    "cassandra_session = cassandra_client.connect()\n",
    "\n",
    "try:\n",
    "    postgres_client.cursor().execute(\"SELECT 0\")\n",
    "    mariadb_client.cursor(buffered=True).execute(\"SELECT 1\")\n",
    "    mysql_client.cursor(buffered=True).execute(\"SELECT 1\")\n",
    "    cassandra_session.execute(\"SELECT release_version FROM system.local\")\n",
    "    mongo_8_config.admin.command('ping')\n",
    "    mongo_7_config.admin.command('ping')\n",
    "except Exception as e:\n",
    "    print(\"connection test failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae52e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_postgres_schema(conn, schema):\n",
    "    if not schema:\n",
    "        return\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(schema)\n",
    "        conn.commit()\n",
    "        print(\"PostgreSQL schema initialization complete.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(\"Error initializing PostgreSQL schema\")\n",
    "\n",
    "\n",
    "def verify_postgres_tables(conn, expected_tables):\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = 'public' AND table_name = ANY(%s);\n",
    "            \"\"\", (expected_tables,))\n",
    "            existing_tables = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "        missing_tables = set(expected_tables) - existing_tables\n",
    "        if not missing_tables:\n",
    "            print(f\"INFO: All PostgreSQL tables exist: {', '.join(expected_tables)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing PostgreSQL tables: {', '.join(missing_tables)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error verifying PostgreSQL tables: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de323680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL schema initialization complete.\n",
      "INFO: All PostgreSQL tables exist: titles, aka_titles, ratings, people, principals, episodes, title_genres\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('db/postgres/schema.sql', 'r') as f:\n",
    "    sql_schema = f.read()\n",
    "\n",
    "postgres_tables = ['titles', 'aka_titles', 'ratings', 'people', 'principals', 'episodes', 'title_genres']\n",
    "\n",
    "initialize_postgres_schema(postgres_client, sql_schema)\n",
    "verify_postgres_tables(postgres_client, postgres_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9295428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_operation(conn, query, fetch=False):\n",
    "    \"\"\"\n",
    "    Execute a PostgreSQL query and optionally fetch results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    conn : psycopg2 connection\n",
    "        Database connection\n",
    "    query : str\n",
    "        SQL query to execute\n",
    "    fetch : bool\n",
    "        Whether to fetch and return results\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Result set if fetch=True, None otherwise\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query)\n",
    "        if fetch:\n",
    "            return cur.fetchall()\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def load_insert_data_from_tsv(data_dir, max_rows=100):\n",
    "    \"\"\"\n",
    "    Load data from TSV files for INSERT benchmark operations.\n",
    "    Loads a subset of rows from each TSV file to use for INSERT benchmarks.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : str or Path\n",
    "        Directory containing TSV files\n",
    "    max_rows : int\n",
    "        Maximum number of rows to load from each file for INSERT operations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Dictionary containing test data for each table\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import csv\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    result = {\n",
    "        'titles': {},\n",
    "        'people': {},\n",
    "        'ratings': {},\n",
    "        'principals': {},\n",
    "        'aka_titles': {},\n",
    "        'episodes': {},\n",
    "        'title_genres': {}\n",
    "    }\n",
    "    \n",
    "    # Helper to escape SQL strings\n",
    "    def escape_sql(s):\n",
    "        if s is None or s == '\\\\N':\n",
    "            return None\n",
    "        return str(s).replace(\"'\", \"''\")\n",
    "    \n",
    "    # Load titles\n",
    "    titles_file = data_path / 'title.basics.tsv'\n",
    "    if titles_file.exists():\n",
    "        with open(titles_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                tconst = row.get('tconst', '')\n",
    "                if tconst:\n",
    "                    result['titles'][tconst] = {\n",
    "                        'title_type': escape_sql(row.get('titleType', '')),\n",
    "                        'primary_title': escape_sql(row.get('primaryTitle', '')),\n",
    "                        'original_title': escape_sql(row.get('originalTitle', '')),\n",
    "                        'is_adult': row.get('isAdult', '0') == '1',\n",
    "                        'start_year': row.get('startYear', '') if row.get('startYear') != '\\\\N' else None,\n",
    "                        'end_year': row.get('endYear', '') if row.get('endYear') != '\\\\N' else None,\n",
    "                        'runtime_minutes': row.get('runtimeMinutes', '') if row.get('runtimeMinutes') != '\\\\N' else None\n",
    "                    }\n",
    "    \n",
    "    # Load people\n",
    "    people_file = data_path / 'name.basics.tsv'\n",
    "    if people_file.exists():\n",
    "        with open(people_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                nconst = row.get('nconst', '')\n",
    "                if nconst:\n",
    "                    result['people'][nconst] = {\n",
    "                        'primary_name': escape_sql(row.get('primaryName', '')),\n",
    "                        'birth_year': row.get('birthYear', '') if row.get('birthYear') != '\\\\N' else None,\n",
    "                        'death_year': row.get('deathYear', '') if row.get('deathYear') != '\\\\N' else None,\n",
    "                        'primary_profession': escape_sql(row.get('primaryProfession', ''))\n",
    "                    }\n",
    "    \n",
    "    # Load ratings\n",
    "    ratings_file = data_path / 'title.ratings.tsv'\n",
    "    if ratings_file.exists():\n",
    "        with open(ratings_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                tconst = row.get('tconst', '')\n",
    "                if tconst:\n",
    "                    result['ratings'][tconst] = {\n",
    "                        'average_rating': row.get('averageRating', '0'),\n",
    "                        'num_votes': row.get('numVotes', '0')\n",
    "                    }\n",
    "    \n",
    "    # Load principals\n",
    "    principals_file = data_path / 'title.principals.tsv'\n",
    "    if principals_file.exists():\n",
    "        with open(principals_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                key = f\"{row.get('tconst', '')}_{row.get('ordering', '')}\"\n",
    "                result['principals'][key] = {\n",
    "                    'title_id': row.get('tconst', ''),\n",
    "                    'ordering': row.get('ordering', '0'),\n",
    "                    'person_id': row.get('nconst', ''),\n",
    "                    'category': escape_sql(row.get('category', '')),\n",
    "                    'job': escape_sql(row.get('job', '')) if row.get('job') != '\\\\N' else None,\n",
    "                    'characters': escape_sql(row.get('characters', '')) if row.get('characters') != '\\\\N' else None\n",
    "                }\n",
    "    \n",
    "    # Load aka_titles\n",
    "    aka_file = data_path / 'title.akas.tsv'\n",
    "    if aka_file.exists():\n",
    "        with open(aka_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                key = f\"{row.get('titleId', '')}_{row.get('ordering', '')}\"\n",
    "                result['aka_titles'][key] = {\n",
    "                    'title_id': row.get('titleId', ''),\n",
    "                    'ordering': row.get('ordering', '0'),\n",
    "                    'aka_title': escape_sql(row.get('title', '')),\n",
    "                    'region': escape_sql(row.get('region', '')) if row.get('region') != '\\\\N' else None,\n",
    "                    'language': escape_sql(row.get('language', '')) if row.get('language') != '\\\\N' else None,\n",
    "                    'types': escape_sql(row.get('types', '')) if row.get('types') != '\\\\N' else None,\n",
    "                    'attributes': escape_sql(row.get('attributes', '')) if row.get('attributes') != '\\\\N' else None,\n",
    "                    'is_original_title': row.get('isOriginalTitle', '0') == '1'\n",
    "                }\n",
    "    \n",
    "    # Load episodes\n",
    "    episodes_file = data_path / 'title.episode.tsv'\n",
    "    if episodes_file.exists():\n",
    "        with open(episodes_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                episode_id = row.get('tconst', '')\n",
    "                if episode_id:\n",
    "                    result['episodes'][episode_id] = {\n",
    "                        'parent_id': row.get('parentTconst', ''),\n",
    "                        'season_number': row.get('seasonNumber', '') if row.get('seasonNumber') != '\\\\N' else None,\n",
    "                        'episode_number': row.get('episodeNumber', '') if row.get('episodeNumber') != '\\\\N' else None\n",
    "                    }\n",
    "    \n",
    "    # Load title_genres (from title.basics.tsv genres column)\n",
    "    if titles_file.exists():\n",
    "        with open(titles_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                tconst = row.get('tconst', '')\n",
    "                genres = row.get('genres', '')\n",
    "                if tconst and genres and genres != '\\\\N':\n",
    "                    for genre in genres.split(','):\n",
    "                        genre = genre.strip()\n",
    "                        if genre:\n",
    "                            key = f\"{tconst}_{genre}\"\n",
    "                            result['title_genres'][key] = {\n",
    "                                'title_id': tconst,\n",
    "                                'genre': genre\n",
    "                            }\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bbab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import statistics\n",
    "from typing import List, Tuple, Callable\n",
    "\n",
    "class BenchmarkTime:    \n",
    "    def __init__(self, db_type: str, data_dir: str):\n",
    "        self.data_dir = data_dir\n",
    "        self.db_type = db_type\n",
    "        self.process = psutil.Process()\n",
    "        self.results = []\n",
    "    \n",
    "    def get_results_df(self):\n",
    "        import pandas as pd\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "    def run_benchmark(self, scenarios: List[Tuple[str, List[Tuple[str, Callable]]]], \n",
    "                    setup_method: Callable = None, \n",
    "                    cleanup_method: Callable = None):\n",
    "        io_counters_start = psutil.disk_io_counters()\n",
    "        \n",
    "        if setup_method:\n",
    "            setup_method()\n",
    "            \n",
    "        for scenario_name, operations in scenarios:\n",
    "            start_time = time.time()\n",
    "            cpu_samples = []\n",
    "            memory_samples = []\n",
    "            durations = []\n",
    "            \n",
    "            for op_name, func in operations:\n",
    "                cpu_samples.append(self.process.cpu_percent())\n",
    "                memory_samples.append(self.process.memory_info().rss)\n",
    "                \n",
    "                op_start = time.time()\n",
    "                func()\n",
    "                op_duration = time.time() - op_start\n",
    "                durations.append(op_duration)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time = end_time - start_time\n",
    "\n",
    "            avg_cpu = statistics.mean(cpu_samples) if cpu_samples else 0\n",
    "            avg_memory = statistics.mean(memory_samples) / (1024 * 1024) if memory_samples else 0\n",
    "            \n",
    "            io_counters_end = psutil.disk_io_counters()\n",
    "            read_mb = (io_counters_end.read_bytes - io_counters_start.read_bytes) / (1024 * 1024)\n",
    "            write_mb = (io_counters_end.write_bytes - io_counters_start.write_bytes) / (1024 * 1024)\n",
    "            \n",
    "            avg_op_time = statistics.mean(durations) if durations else 0\n",
    "            throughput = len(operations) / total_time if total_time > 0 else 0\n",
    "            \n",
    "            scenario_result = {\n",
    "                'database': self.db_type,\n",
    "                'data_dir': self.data_dir,\n",
    "                'scenario': scenario_name,\n",
    "                'total_time': total_time,\n",
    "                'operations': len(operations),\n",
    "                'avg_operation_time': avg_op_time,\n",
    "                'throughput': throughput,\n",
    "                'cpu_avg': avg_cpu,\n",
    "                'memory_avg': avg_memory,\n",
    "                'disk_read_mb': read_mb,\n",
    "                'disk_write_mb': write_mb\n",
    "            }\n",
    "            self.results.append(scenario_result)\n",
    "            \n",
    "            print(f\"--- {scenario_name} ({self.db_type}) ---\")\n",
    "            print(f\"Total time: {total_time:.4f} seconds\")\n",
    "            print(f\"Operations: {len(operations)}\")\n",
    "            print(f\"Avg operation time: {avg_op_time:.4f} seconds\")\n",
    "            print(f\"Throughput: {throughput:.2f} ops/sec\")\n",
    "            print(f\"CPU avg: {avg_cpu:.2f}%\")\n",
    "            print(f\"Memory avg: {avg_memory:.2f} MB\")\n",
    "            print(f\"Disk read: {read_mb:.2f} MB\")\n",
    "            print(f\"Disk write: {write_mb:.2f} MB\")\n",
    "            print()\n",
    "            \n",
    "            io_counters_start = io_counters_end\n",
    "        \n",
    "        if cleanup_method:\n",
    "            cleanup_method()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2efcd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERT_TITLE=\"INSERT Title ?\"\n",
    "INSERT_PERSON=\"INSERT Person ?\"\n",
    "INSERT_RATING=\"INSERT Rating ?\"\n",
    "INSERT_PRINCIPAL=\"INSERT Principal ?\"\n",
    "INSERT_AKA_TITLE=\"INSERT AKA Title ?\"\n",
    "INSERT_EPISODE=\"INSERT Episode ?\"\n",
    "INSERT_TITLE_GENRE=\"INSERT Title Genre ?\"\n",
    "\n",
    "SELECT_TITLE = \"SELECT title ?\"\n",
    "SELECT_PERSON = \"SELECT person ?\"\n",
    "SELECT_ALL_PEOPLE_IN_TITLE = \"SELECT all people that are in the title ?\"\n",
    "SELECT_ALL_EPISODES_FOR_SERIES = \"SELECT all episodes for the series ?\"\n",
    "SELECT_ALL_RATINGS_WITH_TITLE_INFO = \"SELECT all ratings with title info for all titles in the genre ?\"\n",
    "\n",
    "UPDATE_TITLE_PRIMARY_TITLE = \"UPDATE Title ? Primary Title\"\n",
    "UPDATE_ALL_RATINGS_FOR_TITLE = \"UPDATE all Ratings for Title ?\"\n",
    "UPDATE_PERSON_PRIMARY_NAME = \"UPDATE Person ? Primary Name\"\n",
    "UPDATE_TITLE_START_YEAR = \"UPDATE Title ? Start Year\"\n",
    "UPDATE_PERSON_BIRTH_YEAR_FOR_ALL_PEOPLE_IN_TITLE = \"UPDATE Person Birth Year for all people that are in the title ?\"\n",
    "\n",
    "DELETE_TITLE = \"DELETE Title ?\"\n",
    "DELETE_PERSON = \"DELETE Person ?\"\n",
    "DELETE_GENRES_THAT_ARE_IN_THE_TITLE = \"DELETE Genres that are in the title ?\"\n",
    "DELETE_PEOPLE_WHO_ARE_IN_TITLE = \"DELETE People who are in title ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c5c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_postgres_data(conn, data_dir):\n",
    "    \"\"\"\n",
    "    Loads data from TSV files into PostgreSQL tables for IMDB dataset.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Helper function to load TSV using COPY\n",
    "    def copy_from_tsv(conn, table_name, file_path, columns=None):\n",
    "        op_time = time.time()\n",
    "        f_op_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                # Open file and use COPY FROM\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    # Skip header line\n",
    "                    next(f)\n",
    "                    f_op_time = time.time()\n",
    "                    \n",
    "                    if columns:\n",
    "                        cur.copy_from(f, table_name, columns=columns, null='\\\\N', sep='\\t')\n",
    "                    else:\n",
    "                        cur.copy_from(f, table_name, null='\\\\N', sep='\\t')\n",
    "                conn.commit()\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            print(f\"ERROR loading {table_name}: {e}\")\n",
    "            raise\n",
    "        \n",
    "        end_time = time.time()\n",
    "        return op_time, f_op_time, end_time\n",
    "    \n",
    "    # Load titles (from title.basics.tsv)\n",
    "    # Columns: tconst, titleType, primaryTitle, originalTitle, isAdult, startYear, endYear, runtimeMinutes, genres\n",
    "    titles_file = data_path / 'title.basics.tsv'\n",
    "    if titles_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn, 'titles', titles_file,\n",
    "            columns=('tconst', 'title_type', 'primary_title', 'original_title', 'is_adult', 'start_year', 'end_year', 'runtime_minutes')\n",
    "        )\n",
    "        print(f\"INFO: Inserted titles in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    # Load people (from name.basics.tsv)\n",
    "    # Columns: nconst, primaryName, birthYear, deathYear, primaryProfession, knownForTitles\n",
    "    people_file = data_path / 'name.basics.tsv'\n",
    "    if people_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn, 'people', people_file,\n",
    "            columns=('nconst', 'primary_name', 'birth_year', 'death_year', 'primary_profession')\n",
    "        )\n",
    "        print(f\"INFO: Inserted people in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    # Load ratings (from title.ratings.tsv)\n",
    "    # Columns: tconst, averageRating, numVotes\n",
    "    ratings_file = data_path / 'title.ratings.tsv'\n",
    "    if ratings_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn, 'ratings', ratings_file,\n",
    "            columns=('title_id', 'average_rating', 'num_votes')\n",
    "        )\n",
    "        print(f\"INFO: Inserted ratings in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    # Load principals (from title.principals.tsv)\n",
    "    # Columns: tconst, ordering, nconst, category, job, characters\n",
    "    principals_file = data_path / 'title.principals.tsv'\n",
    "    if principals_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn, 'principals', principals_file,\n",
    "            columns=('title_id', 'ordering', 'person_id', 'category', 'job', 'characters')\n",
    "        )\n",
    "        print(f\"INFO: Inserted principals in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    # Load aka_titles (from title.akas.tsv)\n",
    "    # Columns: titleId, ordering, title, region, language, types, attributes, isOriginalTitle\n",
    "    aka_titles_file = data_path / 'title.akas.tsv'\n",
    "    if aka_titles_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn, 'aka_titles', aka_titles_file,\n",
    "            columns=('title_id', 'ordering', 'aka_title', 'region', 'language', 'types', 'attributes', 'is_original_title')\n",
    "        )\n",
    "        print(f\"INFO: Inserted aka_titles in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    # Load episodes (from title.episode.tsv)\n",
    "    # Columns: tconst, parentTconst, seasonNumber, episodeNumber\n",
    "    episodes_file = data_path / 'title.episode.tsv'\n",
    "    if episodes_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn, 'episodes', episodes_file,\n",
    "            columns=('episode_id', 'parent_id', 'season_number', 'episode_number')\n",
    "        )\n",
    "        print(f\"INFO: Inserted episodes in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    # Load title_genres (from title.basics.tsv - genres column needs to be split)\n",
    "    # This requires special handling as genres are comma-separated in the TSV\n",
    "    if titles_file.exists():\n",
    "        op_time = time.time()\n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                with open(titles_file, 'r', encoding='utf-8') as f:\n",
    "                    # Skip header\n",
    "                    next(f)\n",
    "                    f_op_time = time.time()\n",
    "                    \n",
    "                    genre_count = 0\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split('\\t')\n",
    "                        if len(parts) >= 9:\n",
    "                            tconst = parts[0]\n",
    "                            genres = parts[8] if parts[8] != '\\\\N' else ''\n",
    "                            if genres:\n",
    "                                for genre in genres.split(','):\n",
    "                                    genre = genre.strip()\n",
    "                                    if genre:\n",
    "                                        cur.execute(\n",
    "                                            \"INSERT INTO title_genres (title_id, genre) VALUES (%s, %s) ON CONFLICT DO NOTHING\",\n",
    "                                            (tconst, genre)\n",
    "                                        )\n",
    "                                        genre_count += 1\n",
    "                conn.commit()\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            print(f\"ERROR loading title_genres: {e}\")\n",
    "            raise\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"INFO: Inserted {genre_count} title_genres in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_benchmark(data_dir, random_numbers_list) -> pd.DataFrame:\n",
    "    benchmark = BenchmarkTime(\"postgres\", data_dir)\n",
    "    def setup_for_insert():\n",
    "        with open('db/postgres/schema.sql', 'r') as f:\n",
    "            sql_schema = f.read()\n",
    "\n",
    "        initialize_postgres_schema(postgres_client, sql_schema)\n",
    "        load_postgres_data(postgres_client, data_dir)\n",
    "\n",
    "    insert_scenarios = [\n",
    "        (\n",
    "            INSERT_TITLE,\n",
    "            [\n",
    "                (INSERT_TITLE + str(k), lambda k=k, v=v: postgres_operation(postgres_client, \n",
    "                    f\"INSERT INTO titles (tconst, title_type, primary_title, original_title, is_adult, start_year, end_year, runtime_minutes) VALUES ('{k}', '{v['title_type']}', '{v['primary_title']}', '{v['original_title']}', {v['is_adult']}, {v['start_year']}, {v['end_year']}, {v['runtime_minutes']})\")) \n",
    "                for k, v in test_titles.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_PERSON,\n",
    "            [\n",
    "                (INSERT_PERSON + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO people (nconst, primary_name, birth_year, death_year, primary_profession) VALUES ('{k}', '{v['primary_name']}', {v['birth_year']}, {v['death_year']}, '{v['primary_profession']}')\"))\n",
    "                for k, v in test_people.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_RATING,\n",
    "            [\n",
    "                (INSERT_RATING + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO ratings (title_id, average_rating, num_votes) VALUES ('{k}', {v['average_rating']}, {v['num_votes']})\"))\n",
    "                for k, v in test_ratings.items()\n",
    "            ]\n",
    "        ), \n",
    "        (\n",
    "            INSERT_PRINCIPAL,\n",
    "            [\n",
    "                (INSERT_PRINCIPAL + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO principals (title_id, ordering, person_id, category, job, characters) VALUES ('{v['title_id']}', {v['ordering']}, '{v['person_id']}', '{v['category']}', \" + (f\"'{v['job']}'\" if v.get('job') else 'NULL') + \", \" + (f\"'{v['characters']}'\" if v.get('characters') else 'NULL') + \")\"))\n",
    "                for k, v in test_principals.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_AKA_TITLE,\n",
    "            [\n",
    "                (INSERT_AKA_TITLE + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO aka_titles (title_id, ordering, aka_title, region, language, types, attributes, is_original_title) VALUES ('{v['title_id']}', {v['ordering']}, '{v['aka_title']}', \" + (f\"'{v['region']}'\" if v.get('region') else 'NULL') + \", \" + (f\"'{v['language']}'\" if v.get('language') else 'NULL') + \", \" + (f\"'{v['types']}'\" if v.get('types') else 'NULL') + \", \" + (f\"'{v['attributes']}'\" if v.get('attributes') else 'NULL') + f\", {v['is_original_title']})\"))\n",
    "                for k, v in test_aka_titles.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_EPISODE,\n",
    "            [\n",
    "                (INSERT_EPISODE + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO episodes (episode_id, parent_id, season_number, episode_number) VALUES ('{k}', '{v['parent_id']}', {v['season_number']}, {v['episode_number']})\"))\n",
    "                for k, v in test_episodes.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_TITLE_GENRE,\n",
    "            [\n",
    "                (INSERT_TITLE_GENRE + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO title_genres (title_id, genre) VALUES ('{v['title_id']}', '{v['genre']}')\"))\n",
    "                for k, v in test_title_genres.items()\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    select_scenarios = [\n",
    "        (\n",
    "            SELECT_TITLE, [\n",
    "                (SELECT_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM titles WHERE tconst = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_PERSON, [\n",
    "                (SELECT_PERSON + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM people WHERE nconst = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_PEOPLE_IN_TITLE, [\n",
    "                (SELECT_ALL_PEOPLE_IN_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM people p JOIN principals pr ON p.nconst = pr.person_id WHERE pr.title_id = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_EPISODES_FOR_SERIES, [\n",
    "                (SELECT_ALL_EPISODES_FOR_SERIES + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM episodes e JOIN titles t ON e.episode_id = t.tconst WHERE e.parent_id = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ] \n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_RATINGS_WITH_TITLE_INFO, [\n",
    "                (SELECT_ALL_RATINGS_WITH_TITLE_INFO + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM ratings r JOIN titles t ON r.title_id = t.tconst JOIN title_genres tg ON t.tconst = tg.title_id WHERE tg.genre = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ] \n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    update_scenarios = [\n",
    "        (\n",
    "            UPDATE_TITLE_PRIMARY_TITLE, [\n",
    "                (UPDATE_TITLE_PRIMARY_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE titles SET primary_title = 'UPDATED' WHERE tconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_ALL_RATINGS_FOR_TITLE, [\n",
    "                (UPDATE_ALL_RATINGS_FOR_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE ratings SET average_rating = 10.0 WHERE title_id = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_PERSON_PRIMARY_NAME, [\n",
    "                (UPDATE_PERSON_PRIMARY_NAME + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE people SET primary_name = 'UPDATED' WHERE nconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_TITLE_START_YEAR, [\n",
    "                (UPDATE_TITLE_START_YEAR + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE titles SET start_year = 2024 WHERE tconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_PERSON_BIRTH_YEAR_FOR_ALL_PEOPLE_IN_TITLE, [\n",
    "                (UPDATE_PERSON_BIRTH_YEAR_FOR_ALL_PEOPLE_IN_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE people SET birth_year = 2000 WHERE nconst IN (SELECT person_id FROM principals WHERE title_id = '{k}')\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    delete_scenarios = [\n",
    "        (\n",
    "            DELETE_TITLE, [\n",
    "                (DELETE_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM titles WHERE tconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_PERSON, [\n",
    "                (DELETE_PERSON + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM people WHERE nconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_GENRES_THAT_ARE_IN_THE_TITLE, [\n",
    "                (DELETE_GENRES_THAT_ARE_IN_THE_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM title_genres WHERE title_id = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_PEOPLE_WHO_ARE_IN_TITLE, [\n",
    "                (DELETE_PEOPLE_WHO_ARE_IN_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM principals WHERE title_id = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    benchmark.run_benchmark(insert_scenarios, setup_method=setup_for_insert)\n",
    "    benchmark.run_benchmark(select_scenarios)\n",
    "    benchmark.run_benchmark(update_scenarios)\n",
    "    benchmark.run_benchmark(delete_scenarios)\n",
    "    return benchmark.get_results_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7516f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG=False\n",
    "from random import sample\n",
    "def run_all_benchmarks(scale, ignore_cassandra=False):\n",
    "    data_path = './data/scale_' + str(scale)\n",
    "\n",
    "    # generate_files(output_dir=data_path, scale=adjusted_scale)\n",
    "    rand_list = sample(range(1, adjusted_scale+1), 10)\n",
    "\n",
    "    results = []\n",
    "    postgres_results_df = postgres_benchmark(data_path, rand_list)\n",
    "    results.append(postgres_results_df)\n",
    "\n",
    "    merged_df = pd.concat(results, ignore_index=True)\n",
    "    merged_df.to_csv(f\"{data_path}/merged_results.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a763c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_benchmark_for_databases\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m run_benchmark_for_databases(\u001b[38;5;241m100000\u001b[39m)\n\u001b[0;32m      3\u001b[0m run_benchmark_for_databases(\u001b[38;5;241m1000000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mrun_benchmark_for_databases\u001b[1;34m(scale, ignore_cassandra)\u001b[0m\n\u001b[0;32m      4\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/scale_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(scale)\n\u001b[0;32m      6\u001b[0m adjusted_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(scale\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mgenerate_files\u001b[49m(output_dir\u001b[38;5;241m=\u001b[39mdata_path, scale\u001b[38;5;241m=\u001b[39madjusted_scale, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n\u001b[0;32m      8\u001b[0m rand_list \u001b[38;5;241m=\u001b[39m sample(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, adjusted_scale\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     10\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate_files' is not defined"
     ]
    }
   ],
   "source": [
    "run_all_benchmarks(10000)\n",
    "run_all_benchmarks(100000)\n",
    "run_all_benchmarks(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7191900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
