{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fb0bfa3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T18:25:33.541201Z",
     "start_time": "2025-11-26T18:25:33.361561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection has been established\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import psycopg2.errors\n",
    "from pymongo import MongoClient\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra import OperationTimedOut\n",
    "import mysql.connector\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import psutil\n",
    "import statistics\n",
    "from typing import List, Tuple, Callable, Set\n",
    "with open('docker-compose.yaml', 'r') as file:\n",
    "    docker_config = yaml.safe_load(file)\n",
    "\n",
    "postgres_config = docker_config['services']['postgres']\n",
    "postgres_client = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    database=postgres_config['environment']['POSTGRES_DB'],\n",
    "    user=postgres_config['environment']['POSTGRES_USER'],\n",
    "    password=postgres_config['environment']['POSTGRES_PASSWORD'],\n",
    "    port=postgres_config['ports'][0].split(':')[0]\n",
    ")\n",
    "\n",
    "mariadb_config = docker_config['services']['mariadb']\n",
    "mariadb_client = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    database=mariadb_config['environment']['MYSQL_DATABASE'],\n",
    "    user=mariadb_config['environment']['MYSQL_USER'],\n",
    "    password=mariadb_config['environment']['MYSQL_PASSWORD'],\n",
    "    port=mariadb_config['ports'][0].split(':')[0],\n",
    "    allow_local_infile=True\n",
    ")\n",
    "\n",
    "mysql_config = docker_config['services']['mysql']\n",
    "mysql_client = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    database=mysql_config['environment']['MYSQL_DATABASE'],\n",
    "    user=mysql_config['environment']['MYSQL_USER'],\n",
    "    password=mysql_config['environment']['MYSQL_PASSWORD'],\n",
    "    port=mysql_config['ports'][0].split(':')[0],\n",
    "    allow_local_infile=True\n",
    ")\n",
    "\n",
    "mongo_8_config = docker_config['services']['mongo-8']\n",
    "mongo_8_config = MongoClient(\n",
    "    host='localhost',\n",
    "    port=int(mongo_8_config['ports'][0].split(':')[0])\n",
    ")\n",
    "\n",
    "mongo_7_config = docker_config['services']['mongo-7']\n",
    "mongo_7_config = MongoClient(\n",
    "    host='localhost',\n",
    "    port=int(mongo_7_config['ports'][0].split(':')[0])\n",
    ")\n",
    "\n",
    "cassandra_config = docker_config['services']['cassandra']\n",
    "cassandra_client = Cluster(['localhost'], port=int(cassandra_config['ports'][0].split(':')[0]))\n",
    "cassandra_session = cassandra_client.connect()\n",
    "\n",
    "try:\n",
    "    postgres_client.cursor().execute(\"SELECT 0\")\n",
    "    mariadb_client.cursor(buffered=True).execute(\"SELECT 1\")\n",
    "    mysql_client.cursor(buffered=True).execute(\"SELECT 1\")\n",
    "    cassandra_session.execute(\"SELECT release_version FROM system.local\")\n",
    "    mongo_8_config.admin.command('ping')\n",
    "    mongo_7_config.admin.command('ping')\n",
    "    print(\"connection has been established\")\n",
    "except Exception as e:\n",
    "    print(\"connection test failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5ae52e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_postgres_schema(conn, schema):\n",
    "    if not schema:\n",
    "        return\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(schema)\n",
    "        conn.commit()\n",
    "        print(\"PostgreSQL schema initialization complete.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(\"Error initializing PostgreSQL schema\")\n",
    "\n",
    "\n",
    "def verify_postgres_tables(conn, expected_tables):\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = 'public' AND table_name = ANY(%s);\n",
    "            \"\"\", (expected_tables,))\n",
    "            existing_tables = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "        missing_tables = set(expected_tables) - existing_tables\n",
    "        if not missing_tables:\n",
    "            print(f\"INFO: All PostgreSQL tables exist: {', '.join(expected_tables)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing PostgreSQL tables: {', '.join(missing_tables)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error verifying PostgreSQL tables: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "07b64d6c-9a19-4872-8290-894a875e4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_mysql_schema(conn, schema: str):\n",
    "    if not schema:\n",
    "        return\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # opcjonalnie, jak chcesz mieć pewność że FK nie przeszkadzają przy DROP TABLE:\n",
    "        # cur.execute(\"SET FOREIGN_KEY_CHECKS = 0;\")\n",
    "        \n",
    "        statements = schema.split(';')\n",
    "        for statement in statements:\n",
    "            stmt = statement.strip()\n",
    "            if not stmt:\n",
    "                continue\n",
    "            # debugowo możesz sobie wydrukować:\n",
    "            # print(\"EXECUTING:\", stmt)\n",
    "            cur.execute(stmt)\n",
    "\n",
    "        # cur.execute(\"SET FOREIGN_KEY_CHECKS = 1;\")\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"MySQL schema initialization complete.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error initializing MySQL schema: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def verify_mysql_tables(conn, expected_tables):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        placeholders = ','.join(['%s'] * len(expected_tables))\n",
    "        query = f\"\"\"\n",
    "            SELECT table_name\n",
    "            FROM information_schema.tables\n",
    "            WHERE table_schema = DATABASE()\n",
    "              AND table_name IN ({placeholders});\n",
    "        \"\"\"\n",
    "        cur.execute(query, tuple(expected_tables))\n",
    "        existing_tables = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "        missing_tables = set(expected_tables) - existing_tables\n",
    "        if not missing_tables:\n",
    "            print(f\"INFO: All MySQL tables exist: {', '.join(expected_tables)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing MySQL tables: {', '.join(missing_tables)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error verifying MySQL tables: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a6ad1809-5496-48ff-95b8-6692167a771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_cassandra_schema(session, schema: str):\n",
    "    \"\"\"\n",
    "    Inicjalizuje schemę w Cassandrze (CQL).\n",
    "    Przyjmuje aktywną sesję Cassandry oraz schemat CQL jako ciąg znaków.\n",
    "    Wykonuje instrukcje CQL sekwencyjnie.\n",
    "    \"\"\"\n",
    "    if not schema:\n",
    "        return\n",
    "\n",
    "    KEYSPACE_NAME = \"imdb\" # Zgodne z wygenerowanym schematem CQL\n",
    "\n",
    "    try:\n",
    "        # Dzielenie schematu na instrukcje na podstawie średnika (analiza analogiczna do MySQL [1]).\n",
    "        statements = schema.split(';')\n",
    "        \n",
    "        for statement in statements:\n",
    "            stmt = statement.strip()\n",
    "            if not stmt:\n",
    "                continue\n",
    "\n",
    "            # Wykonanie instrukcji CQL\n",
    "            session.execute(stmt)\n",
    "\n",
    "        # Upewnienie się, że sesja używa poprawnego Keyspace po jego utworzeniu (instrukcja 'USE imdb;' \n",
    "        # powinna już być w schemacie, ale ta linia zapewnia spójność dla dalszych operacji).\n",
    "        session.set_keyspace(KEYSPACE_NAME)\n",
    "        \n",
    "        print(\"Cassandra schema initialization complete.\")\n",
    "        \n",
    "    except OperationTimedOut as e:\n",
    "        print(f\"Error initializing Cassandra schema: Operation timed out. Upewnij się, że klaster jest gotowy: {e}\")\n",
    "    except Exception as e:\n",
    "        # Cassandra nie używa rollback, ale zgłasza błąd w przypadku problemów z CQL lub połączeniem.\n",
    "        print(f\"Error initializing Cassandra schema: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "de323680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL schema initialization complete.\n",
      "INFO: All PostgreSQL tables exist: titles, aka_titles, ratings, people, principals, episodes, title_genres\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('db/postgres/schema.sql', 'r') as f:\n",
    "    sql_schema = f.read()\n",
    "\n",
    "postgres_tables = ['titles', 'aka_titles', 'ratings', 'people', 'principals', 'episodes', 'title_genres']\n",
    "\n",
    "initialize_postgres_schema(postgres_client, sql_schema)\n",
    "verify_postgres_tables(postgres_client, postgres_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "923607d0-514e-4b27-8139-5017133b7d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL schema initialization complete.\n",
      "INFO: All MySQL tables exist: titles, aka_titles, ratings, people, principals, episodes, title_genres\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('db/mysql/schema.sql', 'r') as f:\n",
    "    mysql_schema = f.read()\n",
    "\n",
    "mysql_tables = ['titles', 'aka_titles', 'ratings', 'people', 'principals', 'episodes', 'title_genres']\n",
    "\n",
    "initialize_mysql_schema(mysql_client, mysql_schema)\n",
    "verify_mysql_tables(mysql_client, mysql_tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5c742a2d-0222-4592-9e0d-1ae3852de5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cassandra schema initialization complete.\n",
      "INFO: All Cassandra tables exist in 'imdb': titles, people, ratings_by_title, principals_by_title, aka_titles, episodes_by_series, title_genres_by_title, ratings_by_genre\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cassandra_tables = ['titles', 'people', 'ratings_by_title', 'principals_by_title', 'aka_titles', 'episodes_by_series', 'title_genres_by_title', 'ratings_by_genre']\n",
    "\n",
    "with open('db/cassandra/schema.cql', 'r') as f:\n",
    "    cassandra_schema = f.read()\n",
    "\n",
    "initialize_cassandra_schema(cassandra_session, cassandra_schema) \n",
    "verify_cassandra_tables(cassandra_session, cassandra_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3e9ffc51-147f-4f06-a150-1cde819f7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_operation(conn, query, fetch=False):\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            if fetch:\n",
    "                return cur.fetchall()\n",
    "        if not fetch:\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Błąd operacji PostgreSQL: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0c3cf889-ad85-4888-9afd-baebd6b131db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysql_operation(conn, query, fetch=False, params=None):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        if params is not None:\n",
    "            cur.execute(query, params)\n",
    "        else:\n",
    "            cur.execute(query)\n",
    "\n",
    "        if fetch:\n",
    "            result = cur.fetchall()\n",
    "        else:\n",
    "            conn.commit()\n",
    "            result = None\n",
    "\n",
    "        cur.close()\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Błąd operacji MySQL: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ccaffe29-5d6c-4161-a865-60b9223d18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cassandra_operation(session, query, params=None, fetch=False):\n",
    "    \"\"\"\n",
    "    Wykonuje operację Cassandra CQL.\n",
    "    Brak transakcji, brak rollback – Cassandra zapisuje od razu (eventually consistent).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if params is not None:\n",
    "            result = session.execute(query, params)\n",
    "        else:\n",
    "            result = session.execute(query)\n",
    "\n",
    "        if fetch:\n",
    "            return list(result)  # zamiana na listę dla kompatybilności z PG/MySQL\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Błąd operacji Cassandra: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b9295428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_insert_data_from_tsv(data_dir):\n",
    "    data_path = Path(data_dir)\n",
    "    result = {\n",
    "        'titles': {},\n",
    "        'people': {},\n",
    "        'ratings': {},\n",
    "        'principals': {},\n",
    "        'aka_titles': {},\n",
    "        'episodes': {},\n",
    "        'title_genres': {}\n",
    "    }\n",
    "    \n",
    "    # Helper to escape SQL strings\n",
    "    def escape_sql(s):\n",
    "        if s is None or s == '\\\\N':\n",
    "            return None\n",
    "        return str(s).replace(\"'\", \"''\")\n",
    "    \n",
    "    # Load titles\n",
    "    titles_file = data_path / 'titles.tsv'\n",
    "    if titles_file.exists():\n",
    "        with open(titles_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                tconst = row.get('tconst', '')\n",
    "                if tconst:\n",
    "                    result['titles'][tconst] = {\n",
    "                        'title_type': escape_sql(row.get('title_type', '')),\n",
    "                        'primary_title': escape_sql(row.get('primary_title', '')),\n",
    "                        'original_title': escape_sql(row.get('original_title', '')),\n",
    "                        'is_adult': row.get('is_adult', '0') == '1',\n",
    "                        'start_year': row.get('start_year', '') if row.get('start_year') != '\\\\N' else None,\n",
    "                        'end_year': row.get('end_year', '') if row.get('end_year') != '\\\\N' else None,\n",
    "                        'runtime_minutes': row.get('runtime_minutes', '') if row.get('runtime_minutes') != '\\\\N' else None\n",
    "                    }\n",
    "    \n",
    "    # Load people\n",
    "    people_file = data_path / 'people.tsv'\n",
    "    if people_file.exists():\n",
    "        with open(people_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                nconst = row.get('nconst', '')\n",
    "                if nconst:\n",
    "                    result['people'][nconst] = {\n",
    "                        'primary_name': escape_sql(row.get('primary_name', '')),\n",
    "                        'birth_year': row.get('birth_year', '') if row.get('birth_year') != '\\\\N' else None,\n",
    "                        'death_year': row.get('death_year', '') if row.get('death_year') != '\\\\N' else None,\n",
    "                        'primary_profession': escape_sql(row.get('primary_profession', ''))\n",
    "                    }\n",
    "    \n",
    "    # Load ratings\n",
    "    ratings_file = data_path / 'ratings.tsv'\n",
    "    if ratings_file.exists():\n",
    "        with open(ratings_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                tconst = row.get('title_id', '')\n",
    "                if tconst:\n",
    "                    result['ratings'][tconst] = {\n",
    "                        'average_rating': row.get('average_rating', '0'),\n",
    "                        'num_votes': row.get('num_votes', '0')\n",
    "                    }\n",
    "    \n",
    "    # Load principals\n",
    "    principals_file = data_path / 'principals.tsv'\n",
    "    if principals_file.exists():\n",
    "        with open(principals_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                title_id = row.get('title_id', '')\n",
    "                ordering = row.get('ordering', '')\n",
    "                key = f\"{title_id}_{ordering}\"\n",
    "                result['principals'][key] = {\n",
    "                    'title_id': title_id,\n",
    "                    'ordering': ordering,\n",
    "                    'person_id': row.get('person_id', ''),\n",
    "                    'category': escape_sql(row.get('category', '')),\n",
    "                    'job': escape_sql(row.get('job', '')) if row.get('job') != '\\\\N' else None,\n",
    "                    'characters': escape_sql(row.get('characters', '')) if row.get('characters') != '\\\\N' else None\n",
    "                }\n",
    "\n",
    "    # Load aka_titles\n",
    "    aka_file = data_path / 'aka_titles.tsv'\n",
    "    if aka_file.exists():\n",
    "        with open(aka_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                title_id = row.get('title_id', '')\n",
    "                ordering = row.get('ordering', '')\n",
    "                key = f\"{title_id}_{ordering}\"\n",
    "                result['aka_titles'][key] = {\n",
    "                    'title_id': title_id,\n",
    "                    'ordering': ordering,\n",
    "                    'aka_title': escape_sql(row.get('aka_title', '')),\n",
    "                    'region': escape_sql(row.get('region', '')) if row.get('region') != '\\\\N' else None,\n",
    "                    'language': escape_sql(row.get('language', '')) if row.get('language') != '\\\\N' else None,\n",
    "                    'types': escape_sql(row.get('types', '')) if row.get('types') != '\\\\N' else None,\n",
    "                    'attributes': escape_sql(row.get('attributes', '')) if row.get('attributes') != '\\\\N' else None,\n",
    "                    'is_original_title': row.get('is_original_title', '0') == '1'\n",
    "                }\n",
    "    \n",
    "    # Load episodes\n",
    "    episodes_file = data_path / 'episodes.tsv'\n",
    "    if episodes_file.exists():\n",
    "        with open(episodes_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                episode_id = row.get('episode_id', '')\n",
    "                result['episodes'][episode_id] = {\n",
    "                    'parent_id': row.get('parent_id', ''),\n",
    "                    'season_number': row.get('season_number', '') if row.get('season_number') != '\\\\N' else None,\n",
    "                    'episode_number': row.get('episode_number', '') if row.get('episode_number') != '\\\\N' else None\n",
    "                }\n",
    "    \n",
    "    # Load title_genres\n",
    "    genres_file = data_path / 'title_genres.tsv'\n",
    "    if genres_file.exists():\n",
    "        with open(genres_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                title_id = row.get('title_id', '')\n",
    "                genre_value = row.get('genre', '')\n",
    "                key = f\"{title_id}_{genre_value}\"\n",
    "                result['title_genres'][key] = {\n",
    "                    'title_id': title_id,\n",
    "                    'genre': genre_value\n",
    "                }    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c2efcd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERT_TITLE=\"INSERT Title ?\"\n",
    "INSERT_PERSON=\"INSERT Person ?\"\n",
    "INSERT_RATING=\"INSERT Rating ?\"\n",
    "INSERT_PRINCIPAL=\"INSERT Principal ?\"\n",
    "INSERT_AKA_TITLE=\"INSERT AKA Title ?\"\n",
    "INSERT_EPISODE=\"INSERT Episode ?\"\n",
    "INSERT_TITLE_GENRE=\"INSERT Title Genre ?\"\n",
    "\n",
    "SELECT_TITLE = \"SELECT title ?\"\n",
    "SELECT_PERSON = \"SELECT person ?\"\n",
    "SELECT_ALL_PEOPLE_IN_TITLE = \"SELECT all people that are in the title ?\"\n",
    "SELECT_ALL_EPISODES_FOR_SERIES = \"SELECT all episodes for the series ?\"\n",
    "SELECT_ALL_RATINGS_WITH_TITLE_INFO = \"SELECT all ratings with title info for all titles in the genre ?\"\n",
    "\n",
    "UPDATE_TITLE_PRIMARY_TITLE = \"UPDATE Title ? Primary Title\"\n",
    "UPDATE_ALL_RATINGS_FOR_TITLE = \"UPDATE all Ratings for Title ?\"\n",
    "UPDATE_PERSON_PRIMARY_NAME = \"UPDATE Person ? Primary Name\"\n",
    "UPDATE_TITLE_START_YEAR = \"UPDATE Title ? Start Year\"\n",
    "UPDATE_PERSON_BIRTH_YEAR_FOR_ALL_PEOPLE_IN_TITLE = \"UPDATE Person Birth Year for all people that are in the title ?\"\n",
    "\n",
    "DELETE_TITLE = \"DELETE Title ?\"\n",
    "DELETE_PERSON = \"DELETE Person ?\"\n",
    "DELETE_GENRES_THAT_ARE_IN_THE_TITLE = \"DELETE Genres that are in the title ?\"\n",
    "DELETE_PEOPLE_WHO_ARE_IN_TITLE = \"DELETE People who are in title ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e1c5c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_postgres_data(conn, data_dir: str):\n",
    "    data_path = Path(data_dir)\n",
    "\n",
    "    def copy_from_tsv(conn, table_name, file_path, columns=None):\n",
    "        op_time = time.time()\n",
    "        f_op_time = time.time()\n",
    "\n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    next(f)\n",
    "                    f_op_time = time.time()\n",
    "\n",
    "                    if columns:\n",
    "                        cur.copy_from(f,\n",
    "                            table_name,\n",
    "                            columns=columns,\n",
    "                            null=\"\\\\N\",\n",
    "                            sep=\"\\t\",\n",
    "                        )\n",
    "                    else:\n",
    "                        cur.copy_from(\n",
    "                            f,\n",
    "                            table_name,\n",
    "                            null=\"\\\\N\",\n",
    "                            sep=\"\\t\",\n",
    "                        )\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            print(f\"ERROR loading {table_name} from {file_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "        end_time = time.time()\n",
    "        return op_time, f_op_time, end_time\n",
    "\n",
    "    # ---------- T I T L E S ----------\n",
    "    titles_file = data_path / \"titles.tsv\"\n",
    "    if titles_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn,\n",
    "            \"titles\",\n",
    "            titles_file,\n",
    "            columns=(\n",
    "                \"tconst\",\n",
    "                \"title_type\",\n",
    "                \"primary_title\",\n",
    "                \"original_title\",\n",
    "                \"is_adult\",\n",
    "                \"start_year\",\n",
    "                \"end_year\",\n",
    "                \"runtime_minutes\",\n",
    "            ),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted titles in {end_time - op_time:.2f} s \"\n",
    "            f\"(COPY phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {titles_file} not found, skipping titles\")\n",
    "\n",
    "    # ---------- P E O P L E ----------\n",
    "    people_file = data_path / \"people.tsv\"\n",
    "    if people_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn,\n",
    "            \"people\",\n",
    "            people_file,\n",
    "            columns=(\n",
    "                \"nconst\",\n",
    "                \"primary_name\",\n",
    "                \"birth_year\",\n",
    "                \"death_year\",\n",
    "                \"primary_profession\",\n",
    "            ),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted people in {end_time - op_time:.2f} s \"\n",
    "            f\"(COPY phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {people_file} not found, skipping people\")\n",
    "\n",
    "    # ---------- R A T I N G S ----------\n",
    "    ratings_file = data_path / \"ratings.tsv\"\n",
    "    if ratings_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn,\n",
    "            \"ratings\",\n",
    "            ratings_file,\n",
    "            columns=(\"title_id\", \"average_rating\", \"num_votes\"),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted ratings in {end_time - op_time:.2f} s \"\n",
    "            f\"(COPY phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {ratings_file} not found, skipping ratings\")\n",
    "\n",
    "    # ---------- P R I N C I P A L S ----------\n",
    "    principals_file = data_path / \"principals.tsv\"\n",
    "    if principals_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn,\n",
    "            \"principals\",\n",
    "            principals_file,\n",
    "            columns=(\"title_id\", \"ordering\", \"person_id\", \"category\", \"job\", \"characters\"),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted principals in {end_time - op_time:.2f} s \"\n",
    "            f\"(COPY phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {principals_file} not found, skipping principals\")\n",
    "\n",
    "    # ---------- A K A _ T I T L E S ----------\n",
    "    aka_titles_file = data_path / \"aka_titles.tsv\"\n",
    "    if aka_titles_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn,\n",
    "            \"aka_titles\",\n",
    "            aka_titles_file,\n",
    "            columns=(\n",
    "                \"title_id\",\n",
    "                \"ordering\",\n",
    "                \"aka_title\",\n",
    "                \"region\",\n",
    "                \"language\",\n",
    "                \"types\",\n",
    "                \"attributes\",\n",
    "                \"is_original_title\",\n",
    "            ),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted aka_titles in {end_time - op_time:.2f} s \"\n",
    "            f\"(COPY phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {aka_titles_file} not found, skipping aka_titles\")\n",
    "\n",
    "    # ---------- E P I S O D E S ----------\n",
    "    episodes_file = data_path / \"episodes.tsv\"\n",
    "    if episodes_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn,\n",
    "            \"episodes\",\n",
    "            episodes_file,\n",
    "            columns=(\"episode_id\", \"parent_id\", \"season_number\", \"episode_number\"),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted episodes in {end_time - op_time:.2f} s \"\n",
    "            f\"(COPY phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {episodes_file} not found, skipping episodes\")\n",
    "\n",
    "    # ---------- T I T L E _ G E N R E S ----------\n",
    "    title_genres_file = data_path / \"title_genres.tsv\"\n",
    "    if title_genres_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn,\n",
    "            \"title_genres\",\n",
    "            title_genres_file,\n",
    "            columns=(\"title_id\", \"genre\"),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted title_genres in {end_time - op_time:.2f} s \"\n",
    "            f\"(COPY phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {title_genres_file} not found, skipping title_genres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dfe073a7-1605-470e-bf57-b276755c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mysql_data(conn, data_dir: str):\n",
    "    data_path = Path(data_dir)\n",
    "\n",
    "    def load_from_tsv(conn, table_name, file_path, columns=None):\n",
    "        op_time = time.time()\n",
    "        f_op_time = time.time()\n",
    "\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            file_path_str = str(file_path.resolve())\n",
    "\n",
    "            # baza opcji – TSV, \\N jako NULL, pierwszy wiersz to nagłówek\n",
    "            base_query = f\"\"\"\n",
    "                LOAD DATA LOCAL INFILE %s\n",
    "                INTO TABLE {table_name}\n",
    "                CHARACTER SET utf8mb4\n",
    "                FIELDS TERMINATED BY '\\t'\n",
    "                ESCAPED BY '\\\\\\\\'\n",
    "                LINES TERMINATED BY '\\n'\n",
    "                IGNORE 1 LINES\n",
    "            \"\"\"\n",
    "\n",
    "            if columns:\n",
    "                cols_sql = \", \".join(columns)\n",
    "                query = base_query + f\" ({cols_sql})\"\n",
    "            else:\n",
    "                query = base_query\n",
    "\n",
    "            f_op_time = time.time()\n",
    "            cur.execute(query, (file_path_str,))\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            print(f\"ERROR loading {table_name} from {file_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "        end_time = time.time()\n",
    "        return op_time, f_op_time, end_time\n",
    "\n",
    "    # ---------- T I T L E S ----------\n",
    "    titles_file = data_path / \"titles.tsv\"\n",
    "    if titles_file.exists():\n",
    "        op_time, f_op_time, end_time = load_from_tsv(\n",
    "            conn,\n",
    "            \"titles\",\n",
    "            titles_file,\n",
    "            columns=(\n",
    "                \"tconst\",\n",
    "                \"title_type\",\n",
    "                \"primary_title\",\n",
    "                \"original_title\",\n",
    "                \"is_adult\",\n",
    "                \"start_year\",\n",
    "                \"end_year\",\n",
    "                \"runtime_minutes\",\n",
    "            ),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted titles in {end_time - op_time:.2f} s \"\n",
    "            f\"(LOAD DATA phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {titles_file} not found, skipping titles\")\n",
    "\n",
    "    # ---------- P E O P L E ----------\n",
    "    people_file = data_path / \"people.tsv\"\n",
    "    if people_file.exists():\n",
    "        op_time, f_op_time, end_time = load_from_tsv(\n",
    "            conn,\n",
    "            \"people\",\n",
    "            people_file,\n",
    "            columns=(\n",
    "                \"nconst\",\n",
    "                \"primary_name\",\n",
    "                \"birth_year\",\n",
    "                \"death_year\",\n",
    "                \"primary_profession\",\n",
    "            ),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted people in {end_time - op_time:.2f} s \"\n",
    "            f\"(LOAD DATA phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {people_file} not found, skipping people\")\n",
    "\n",
    "    # ---------- R A T I N G S ----------\n",
    "    ratings_file = data_path / \"ratings.tsv\"\n",
    "    if ratings_file.exists():\n",
    "        op_time, f_op_time, end_time = load_from_tsv(\n",
    "            conn,\n",
    "            \"ratings\",\n",
    "            ratings_file,\n",
    "            columns=(\"title_id\", \"average_rating\", \"num_votes\"),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted ratings in {end_time - op_time:.2f} s \"\n",
    "            f\"(LOAD DATA phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {ratings_file} not found, skipping ratings\")\n",
    "\n",
    "    # ---------- P R I N C I P A L S ----------\n",
    "    principals_file = data_path / \"principals.tsv\"\n",
    "    if principals_file.exists():\n",
    "        op_time, f_op_time, end_time = load_from_tsv(\n",
    "            conn,\n",
    "            \"principals\",\n",
    "            principals_file,\n",
    "            columns=(\"title_id\", \"ordering\", \"person_id\", \"category\", \"job\", \"characters\"),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted principals in {end_time - op_time:.2f} s \"\n",
    "            f\"(LOAD DATA phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {principals_file} not found, skipping principals\")\n",
    "\n",
    "    # ---------- A K A _ T I T L E S ----------\n",
    "    aka_titles_file = data_path / \"aka_titles.tsv\"\n",
    "    if aka_titles_file.exists():\n",
    "        op_time, f_op_time, end_time = load_from_tsv(\n",
    "            conn,\n",
    "            \"aka_titles\",\n",
    "            aka_titles_file,\n",
    "            columns=(\n",
    "                \"title_id\",\n",
    "                \"ordering\",\n",
    "                \"aka_title\",\n",
    "                \"region\",\n",
    "                \"language\",      # tu musi być dokładnie tak jak w schemacie MySQL\n",
    "                \"types\",\n",
    "                \"attributes\",\n",
    "                \"is_original_title\",\n",
    "            ),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted aka_titles in {end_time - op_time:.2f} s \"\n",
    "            f\"(LOAD DATA phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {aka_titles_file} not found, skipping aka_titles\")\n",
    "\n",
    "    # ---------- E P I S O D E S ----------\n",
    "    episodes_file = data_path / \"episodes.tsv\"\n",
    "    if episodes_file.exists():\n",
    "        op_time, f_op_time, end_time = load_from_tsv(\n",
    "            conn,\n",
    "            \"episodes\",\n",
    "            episodes_file,\n",
    "            columns=(\"episode_id\", \"parent_id\", \"season_number\", \"episode_number\"),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted episodes in {end_time - op_time:.2f} s \"\n",
    "            f\"(LOAD DATA phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {episodes_file} not found, skipping episodes\")\n",
    "\n",
    "    # ---------- T I T L E _ G E N R E S ----------\n",
    "    title_genres_file = data_path / \"title_genres.tsv\"\n",
    "    if title_genres_file.exists():\n",
    "        op_time, f_op_time, end_time = load_from_tsv(\n",
    "            conn,\n",
    "            \"title_genres\",\n",
    "            title_genres_file,\n",
    "            columns=(\"title_id\", \"genre\"),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted title_genres in {end_time - op_time:.2f} s \"\n",
    "            f\"(LOAD DATA phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {title_genres_file} not found, skipping title_genres\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "10ae7813-861a-4b98-93bc-3838fd685339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cassandra_data(session, data_dir: str):\n",
    "    data_path = Path(data_dir)\n",
    "\n",
    "    def parse_int(v):\n",
    "        return int(v) if v not in (None, \"\", \"\\\\N\") else None\n",
    "\n",
    "    def parse_float(v):\n",
    "        return float(v) if v not in (None, \"\", \"\\\\N\") else None\n",
    "\n",
    "    def parse_bool(v):\n",
    "        if v in (\"0\", 0, \"False\", \"false\", \"\\\\N\", None, \"\"):\n",
    "            return False\n",
    "        if v in (\"1\", 1, \"True\", \"true\"):\n",
    "            return True\n",
    "        return None\n",
    "\n",
    "    # ---------- T I T L E S ----------\n",
    "    titles_file = data_path / \"titles.tsv\"\n",
    "    if titles_file.exists():\n",
    "        op_time = time.time()\n",
    "        insert_stmt = session.prepare(\"\"\"\n",
    "            INSERT INTO imdb.titles (\n",
    "                tconst, title_type, primary_title, original_title,\n",
    "                is_adult, start_year, end_year, runtime_minutes\n",
    "            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\")\n",
    "\n",
    "        f_op_time = time.time()\n",
    "        with open(titles_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            next(f)  # skip header\n",
    "            for line in f:\n",
    "                cols = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                session.execute(insert_stmt, (\n",
    "                    cols[0],                     # tconst\n",
    "                    cols[1],                     # title_type\n",
    "                    cols[2],                     # primary_title\n",
    "                    cols[3],                     # original_title\n",
    "                    parse_bool(cols[4]),         # is_adult\n",
    "                    parse_int(cols[5]),          # start_year\n",
    "                    parse_int(cols[6]),          # end_year\n",
    "                    parse_int(cols[7]),          # runtime_minutes\n",
    "                ))\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"INFO: Inserted titles in {end_time - op_time:.2f} s \"\n",
    "            f\"(INSERT phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {titles_file} not found, skipping titles\")\n",
    "\n",
    "    # ---------- P E O P L E ----------\n",
    "    people_file = data_path / \"people.tsv\"\n",
    "    if people_file.exists():\n",
    "        op_time = time.time()\n",
    "        insert_stmt = session.prepare(\"\"\"\n",
    "            INSERT INTO imdb.people (\n",
    "                nconst, primary_name, birth_year, death_year, primary_profession\n",
    "            ) VALUES (?, ?, ?, ?, ?)\n",
    "        \"\"\")\n",
    "\n",
    "        f_op_time = time.time()\n",
    "        with open(people_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                cols = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                session.execute(insert_stmt, (\n",
    "                    cols[0],                     # nconst\n",
    "                    cols[1],                     # primary_name\n",
    "                    parse_int(cols[2]),          # birth_year\n",
    "                    parse_int(cols[3]),          # death_year\n",
    "                    cols[4] if cols[4] != \"\\\\N\" else None,  # primary_profession\n",
    "                ))\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"INFO: Inserted people in {end_time - op_time:.2f} s \"\n",
    "            f\"(INSERT phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {people_file} not found, skipping people\")\n",
    "\n",
    "    # ---------- R A T I N G S ----------\n",
    "    ratings_file = data_path / \"ratings.tsv\"\n",
    "    if ratings_file.exists():\n",
    "        op_time = time.time()\n",
    "        insert_stmt = session.prepare(\"\"\"\n",
    "            INSERT INTO imdb.ratings (\n",
    "                title_id, average_rating, num_votes\n",
    "            ) VALUES (?, ?, ?)\n",
    "        \"\"\")\n",
    "\n",
    "        f_op_time = time.time()\n",
    "        with open(ratings_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                cols = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                session.execute(insert_stmt, (\n",
    "                    cols[0],                     # title_id\n",
    "                    parse_float(cols[1]),        # average_rating\n",
    "                    parse_int(cols[2]),          # num_votes\n",
    "                ))\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"INFO: Inserted ratings in {end_time - op_time:.2f} s \"\n",
    "            f\"(INSERT phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {ratings_file} not found, skipping ratings\")\n",
    "\n",
    "    # ---------- P R I N C I P A L S ----------\n",
    "    principals_file = data_path / \"principals.tsv\"\n",
    "    if principals_file.exists():\n",
    "        op_time = time.time()\n",
    "        insert_stmt = session.prepare(\"\"\"\n",
    "            INSERT INTO imdb.principals (\n",
    "                title_id, ordering, person_id, category, job, characters\n",
    "            ) VALUES (?, ?, ?, ?, ?, ?)\n",
    "        \"\"\")\n",
    "\n",
    "        f_op_time = time.time()\n",
    "        with open(principals_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                cols = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                session.execute(insert_stmt, (\n",
    "                    cols[0],                                 # title_id\n",
    "                    parse_int(cols[1]),                      # ordering\n",
    "                    cols[2],                                 # person_id\n",
    "                    cols[3],                                 # category\n",
    "                    cols[4] if cols[4] != \"\\\\N\" else None,   # job\n",
    "                    cols[5] if cols[5] != \"\\\\N\" else None,   # characters\n",
    "                ))\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"INFO: Inserted principals in {end_time - op_time:.2f} s \"\n",
    "            f\"(INSERT phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {principals_file} not found, skipping principals\")\n",
    "\n",
    "    # ---------- A K A _ T I T L E S ----------\n",
    "    aka_titles_file = data_path / \"aka_titles.tsv\"\n",
    "    if aka_titles_file.exists():\n",
    "        op_time = time.time()\n",
    "        insert_stmt = session.prepare(\"\"\"\n",
    "            INSERT INTO imdb.aka_titles (\n",
    "                title_id, ordering, aka_title, region, language,\n",
    "                types, attributes, is_original_title\n",
    "            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\")\n",
    "\n",
    "        f_op_time = time.time()\n",
    "        with open(aka_titles_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                cols = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                session.execute(insert_stmt, (\n",
    "                    cols[0],                                 # title_id\n",
    "                    parse_int(cols[1]),                      # ordering\n",
    "                    cols[2],                                 # aka_title\n",
    "                    cols[3] if cols[3] != \"\\\\N\" else None,   # region\n",
    "                    cols[4] if cols[4] != \"\\\\N\" else None,   # language\n",
    "                    cols[5] if cols[5] != \"\\\\N\" else None,   # types\n",
    "                    cols[6] if cols[6] != \"\\\\N\" else None,   # attributes\n",
    "                    parse_bool(cols[7]),                     # is_original_title\n",
    "                ))\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"INFO: Inserted aka_titles in {end_time - op_time:.2f} s \"\n",
    "            f\"(INSERT phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {aka_titles_file} not found, skipping aka_titles\")\n",
    "\n",
    "    # ---------- E P I S O D E S ----------\n",
    "    episodes_file = data_path / \"episodes.tsv\"\n",
    "    if episodes_file.exists():\n",
    "        op_time = time.time()\n",
    "        insert_stmt = session.prepare(\"\"\"\n",
    "            INSERT INTO imdb.episodes (\n",
    "                episode_id, parent_id, season_number, episode_number\n",
    "            ) VALUES (?, ?, ?, ?)\n",
    "        \"\"\")\n",
    "\n",
    "        f_op_time = time.time()\n",
    "        with open(episodes_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                cols = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                session.execute(insert_stmt, (\n",
    "                    cols[0],                     # episode_id\n",
    "                    cols[1],                     # parent_id\n",
    "                    parse_int(cols[2]),          # season_number\n",
    "                    parse_int(cols[3]),          # episode_number\n",
    "                ))\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"INFO: Inserted episodes in {end_time - op_time:.2f} s \"\n",
    "            f\"(INSERT phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {episodes_file} not found, skipping episodes\")\n",
    "\n",
    "    # ---------- T I T L E _ G E N R E S ----------\n",
    "    title_genres_file = data_path / \"title_genres.tsv\"\n",
    "    if title_genres_file.exists():\n",
    "        op_time = time.time()\n",
    "        insert_stmt = session.prepare(\"\"\"\n",
    "            INSERT INTO imdb.title_genres (\n",
    "                title_id, genre\n",
    "            ) VALUES (?, ?)\n",
    "        \"\"\")\n",
    "\n",
    "        f_op_time = time.time()\n",
    "        with open(title_genres_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                cols = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                session.execute(insert_stmt, (\n",
    "                    cols[0],     # title_id\n",
    "                    cols[1],     # genre\n",
    "                ))\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"INFO: Inserted title_genres in {end_time - op_time:.2f} s \"\n",
    "            f\"(INSERT phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {title_genres_file} not found, skipping title_genres\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a2bbab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkTime:    \n",
    "    def __init__(self, db_type: str, data_dir: str):\n",
    "        self.data_dir = data_dir\n",
    "        self.db_type = db_type\n",
    "        self.process = psutil.Process()\n",
    "        self.results = []\n",
    "    \n",
    "    def get_results_df(self):\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "    def run_benchmark(self, scenarios: List[Tuple[str, List[Tuple[str, Callable]]]], \n",
    "                    setup_method: Callable = None, \n",
    "                    cleanup_method: Callable = None):\n",
    "        io_counters_start = psutil.disk_io_counters()\n",
    "        \n",
    "        if setup_method:\n",
    "            setup_method()\n",
    "            \n",
    "        for scenario_name, operations in scenarios:\n",
    "            start_time = time.time()\n",
    "            cpu_samples = []\n",
    "            memory_samples = []\n",
    "            durations = []\n",
    "            \n",
    "            for op_name, func in operations:\n",
    "                cpu_samples.append(self.process.cpu_percent())\n",
    "                memory_samples.append(self.process.memory_info().rss)\n",
    "                \n",
    "                op_start = time.time()\n",
    "                func()\n",
    "                op_duration = time.time() - op_start\n",
    "                durations.append(op_duration)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time = end_time - start_time\n",
    "\n",
    "            avg_cpu = statistics.mean(cpu_samples) if cpu_samples else 0\n",
    "            avg_memory = statistics.mean(memory_samples) / (1024 * 1024) if memory_samples else 0\n",
    "            \n",
    "            io_counters_end = psutil.disk_io_counters()\n",
    "            read_mb = (io_counters_end.read_bytes - io_counters_start.read_bytes) / (1024 * 1024)\n",
    "            write_mb = (io_counters_end.write_bytes - io_counters_start.write_bytes) / (1024 * 1024)\n",
    "            \n",
    "            avg_op_time = statistics.mean(durations) if durations else 0\n",
    "            throughput = len(operations) / total_time if total_time > 0 else 0\n",
    "            \n",
    "            scenario_result = {\n",
    "                'database': self.db_type,\n",
    "                'data_dir': self.data_dir,\n",
    "                'scenario': scenario_name,\n",
    "                'total_time': total_time,\n",
    "                'operations': len(operations),\n",
    "                'avg_operation_time': avg_op_time,\n",
    "                'throughput': throughput,\n",
    "                'cpu_avg': avg_cpu,\n",
    "                'memory_avg': avg_memory,\n",
    "                'disk_read_mb': read_mb,\n",
    "                'disk_write_mb': write_mb\n",
    "            }\n",
    "            self.results.append(scenario_result)\n",
    "            \n",
    "            print(f\"--- {scenario_name} ({self.db_type}) ---\")\n",
    "            print(f\"Total time: {total_time:.4f} seconds\")\n",
    "            print(f\"Operations: {len(operations)}\")\n",
    "            print(f\"Avg operation time: {avg_op_time:.4f} seconds\")\n",
    "            print(f\"Throughput: {throughput:.2f} ops/sec\")\n",
    "            print(f\"CPU avg: {avg_cpu:.2f}%\")\n",
    "            print(f\"Memory avg: {avg_memory:.2f} MB\")\n",
    "            print(f\"Disk read: {read_mb:.2f} MB\")\n",
    "            print(f\"Disk write: {write_mb:.2f} MB\")\n",
    "            print()\n",
    "            \n",
    "            io_counters_start = io_counters_end\n",
    "        \n",
    "        if cleanup_method:\n",
    "            cleanup_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9d76a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_benchmark(all_data_dir, data_dir, random_numbers_list) -> pd.DataFrame:\n",
    "    benchmark = BenchmarkTime(\"postgres\", data_dir)\n",
    "    def setup_for_insert():\n",
    "        with open('db/postgres/schema.sql', 'r') as f:\n",
    "            sql_schema = f.read()\n",
    "\n",
    "        initialize_postgres_schema(postgres_client, sql_schema)\n",
    "        load_postgres_data(postgres_client, all_data_dir)\n",
    "\n",
    "    test_data = load_insert_data_from_tsv(data_dir)\n",
    "    test_titles = test_data['titles']\n",
    "    test_people = test_data['people']\n",
    "    test_ratings = test_data['ratings']\n",
    "    test_principals = test_data['principals']\n",
    "    test_aka_titles = test_data['aka_titles']\n",
    "    test_episodes = test_data['episodes']\n",
    "    test_title_genres = test_data['title_genres']\n",
    "    \n",
    "    insert_scenarios = [\n",
    "        (\n",
    "            INSERT_TITLE,\n",
    "            [\n",
    "                (INSERT_TITLE + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO titles (tconst, title_type, primary_title, original_title, is_adult, start_year, end_year, runtime_minutes) VALUES ('{k}', '{v['title_type']}', '{v['primary_title']}', '{v['original_title']}', {v['is_adult']}, \" +\n",
    "                    (f\"{v['start_year']}\" if v.get('start_year') is not None else 'NULL') + \", \" +\n",
    "                    (f\"{v['end_year']}\" if v.get('end_year') is not None else 'NULL') + \", \" +\n",
    "                    (f\"{v['runtime_minutes']}\" if v.get('runtime_minutes') is not None else 'NULL') +\n",
    "                    \")\"))\n",
    "                for k, v in test_titles.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_PERSON,\n",
    "            [\n",
    "                (INSERT_PERSON + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO people (nconst, primary_name, birth_year, death_year, primary_profession) VALUES ('{k}', '{v['primary_name']}', \" +\n",
    "                    (f\"{v['birth_year']}\" if v.get('birth_year') is not None else 'NULL') + \", \" +\n",
    "                    (f\"{v['death_year']}\" if v.get('death_year') is not None else 'NULL') + \", \" +\n",
    "                    f\"'{v['primary_profession']}')\"))\n",
    "                for k, v in test_people.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_RATING,\n",
    "            [\n",
    "                (INSERT_RATING + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO ratings (title_id, average_rating, num_votes) VALUES ('{k}', {v['average_rating']}, {v['num_votes']})\"))\n",
    "                for k, v in test_ratings.items()\n",
    "            ]\n",
    "        ), \n",
    "        (\n",
    "            INSERT_PRINCIPAL,\n",
    "            [\n",
    "                (INSERT_PRINCIPAL + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO principals (title_id, ordering, person_id, category, job, characters) VALUES ('{v['title_id']}', {v['ordering']}, '{v['person_id']}', '{v['category']}', \" + (f\"'{v['job']}'\" if v.get('job') else 'NULL') + \", \" + (f\"'{v['characters']}'\" if v.get('characters') else 'NULL') + \")\"))\n",
    "                for k, v in test_principals.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_AKA_TITLE,\n",
    "            [\n",
    "                (INSERT_AKA_TITLE + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO aka_titles (title_id, ordering, aka_title, region, language, types, attributes, is_original_title) VALUES ('{v['title_id']}', {v['ordering']}, '{v['aka_title']}', \" + (f\"'{v['region']}'\" if v.get('region') else 'NULL') + \", \" + (f\"'{v['language']}'\" if v.get('language') else 'NULL') + \", \" + (f\"'{v['types']}'\" if v.get('types') else 'NULL') + \", \" + (f\"'{v['attributes']}'\" if v.get('attributes') else 'NULL') + f\", {v['is_original_title']})\"))\n",
    "                for k, v in test_aka_titles.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_EPISODE,\n",
    "            [\n",
    "                (INSERT_EPISODE + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO episodes (episode_id, parent_id, season_number, episode_number) VALUES ('{k}', '{v['parent_id']}', \" +\n",
    "                    (f\"{v['season_number']}\" if v.get('season_number') is not None else 'NULL') + \", \" +\n",
    "                    (f\"{v['episode_number']}\" if v.get('episode_number') is not None else 'NULL') +\n",
    "                    \")\"))\n",
    "                for k, v in test_episodes.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_TITLE_GENRE,\n",
    "            [\n",
    "                (INSERT_TITLE_GENRE + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO title_genres (title_id, genre) VALUES ('{v['title_id']}', '{v['genre']}')\"))\n",
    "                for k, v in test_title_genres.items()\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    select_scenarios = [\n",
    "        (\n",
    "            SELECT_TITLE, [\n",
    "                (SELECT_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM titles WHERE tconst = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_PERSON, [\n",
    "                (SELECT_PERSON + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM people WHERE nconst = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_PEOPLE_IN_TITLE, [\n",
    "                (SELECT_ALL_PEOPLE_IN_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM people p JOIN principals pr ON p.nconst = pr.person_id WHERE pr.title_id = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_EPISODES_FOR_SERIES, [\n",
    "                (SELECT_ALL_EPISODES_FOR_SERIES + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM episodes e JOIN titles t ON e.episode_id = t.tconst WHERE e.parent_id = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ] \n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_RATINGS_WITH_TITLE_INFO, [\n",
    "                (SELECT_ALL_RATINGS_WITH_TITLE_INFO + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM ratings r JOIN titles t ON r.title_id = t.tconst JOIN title_genres tg ON t.tconst = tg.title_id WHERE tg.genre = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ] \n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    update_scenarios = [\n",
    "        (\n",
    "            UPDATE_TITLE_PRIMARY_TITLE, [\n",
    "                (UPDATE_TITLE_PRIMARY_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE titles SET primary_title = 'UPDATED' WHERE tconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_ALL_RATINGS_FOR_TITLE, [\n",
    "                (UPDATE_ALL_RATINGS_FOR_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE ratings SET average_rating = 10.0 WHERE title_id = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_PERSON_PRIMARY_NAME, [\n",
    "                (UPDATE_PERSON_PRIMARY_NAME + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE people SET primary_name = 'UPDATED' WHERE nconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_TITLE_START_YEAR, [\n",
    "                (UPDATE_TITLE_START_YEAR + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE titles SET start_year = 2024 WHERE tconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_PERSON_BIRTH_YEAR_FOR_ALL_PEOPLE_IN_TITLE, [\n",
    "                (UPDATE_PERSON_BIRTH_YEAR_FOR_ALL_PEOPLE_IN_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE people SET birth_year = 2000 WHERE nconst IN (SELECT person_id FROM principals WHERE title_id = '{k}')\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    delete_scenarios = [\n",
    "        (\n",
    "            DELETE_TITLE, [\n",
    "                (DELETE_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM titles WHERE tconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_PERSON, [\n",
    "                (DELETE_PERSON + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM people WHERE nconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_GENRES_THAT_ARE_IN_THE_TITLE, [\n",
    "                (DELETE_GENRES_THAT_ARE_IN_THE_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM title_genres WHERE title_id = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_PEOPLE_WHO_ARE_IN_TITLE, [\n",
    "                (DELETE_PEOPLE_WHO_ARE_IN_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM principals WHERE title_id = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    benchmark.run_benchmark(insert_scenarios)\n",
    "    #benchmark.run_benchmark(insert_scenarios, setup_method=setup_for_insert) # TEST do testow bez setup_method\n",
    "    benchmark.run_benchmark(select_scenarios)\n",
    "    benchmark.run_benchmark(update_scenarios)\n",
    "    benchmark.run_benchmark(delete_scenarios)\n",
    "    return benchmark.get_results_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c3c4c06c-3bfd-498b-a363-69469d8cc9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysql_benchmark(all_data_dir, data_dir, random_numbers_list) -> pd.DataFrame:\n",
    "    benchmark = BenchmarkTime(\"mysql\", data_dir)\n",
    "\n",
    "    def setup_for_insert():\n",
    "        with open('db/mysql/schema.sql', 'r') as f:\n",
    "            sql_schema = f.read()\n",
    "\n",
    "        initialize_mysql_schema(mysql_client, sql_schema)\n",
    "        load_mysql_data(mysql_client, all_data_dir)\n",
    "\n",
    "    test_data = load_insert_data_from_tsv(data_dir)\n",
    "    test_titles = test_data['titles']\n",
    "    test_people = test_data['people']\n",
    "    test_ratings = test_data['ratings']\n",
    "    test_principals = test_data['principals']\n",
    "    test_aka_titles = test_data['aka_titles']\n",
    "    test_episodes = test_data['episodes']\n",
    "    test_title_genres = test_data['title_genres']\n",
    "    \n",
    "    insert_scenarios = [\n",
    "        (\n",
    "            INSERT_TITLE,\n",
    "            [\n",
    "                (INSERT_TITLE + str(k), lambda k=k, v=v: mysql_operation(mysql_client,\n",
    "                    f\"INSERT INTO titles (tconst, title_type, primary_title, original_title, is_adult, start_year, end_year, runtime_minutes) VALUES ('{k}', '{v['title_type']}', '{v['primary_title']}', '{v['original_title']}', {v['is_adult']}, \" +\n",
    "                    (f\"{v['start_year']}\" if v.get('start_year') is not None else 'NULL') + \", \" +\n",
    "                    (f\"{v['end_year']}\" if v.get('end_year') is not None else 'NULL') + \", \" +\n",
    "                    (f\"{v['runtime_minutes']}\" if v.get('runtime_minutes') is not None else 'NULL') +\n",
    "                    \")\"))\n",
    "                for k, v in test_titles.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_PERSON,\n",
    "            [\n",
    "                (INSERT_PERSON + str(k), lambda k=k, v=v: mysql_operation(mysql_client,\n",
    "                    f\"INSERT INTO people (nconst, primary_name, birth_year, death_year, primary_profession) VALUES ('{k}', '{v['primary_name']}', \" +\n",
    "                    (f\"{v['birth_year']}\" if v.get('birth_year') is not None else 'NULL') + \", \" +\n",
    "                    (f\"{v['death_year']}\" if v.get('death_year') is not None else 'NULL') + \", \" +\n",
    "                    f\"'{v['primary_profession']}')\"))\n",
    "                for k, v in test_people.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_RATING,\n",
    "            [\n",
    "                (INSERT_RATING + str(k), lambda k=k, v=v: mysql_operation(mysql_client,\n",
    "                    f\"INSERT INTO ratings (title_id, average_rating, num_votes) VALUES ('{k}', {v['average_rating']}, {v['num_votes']})\"))\n",
    "                for k, v in test_ratings.items()\n",
    "            ]\n",
    "        ), \n",
    "        (\n",
    "            INSERT_PRINCIPAL,\n",
    "            [\n",
    "                (INSERT_PRINCIPAL + str(k), lambda k=k, v=v: mysql_operation(mysql_client,\n",
    "                    f\"INSERT INTO principals (title_id, ordering, person_id, category, job, characters) VALUES ('{v['title_id']}', {v['ordering']}, '{v['person_id']}', '{v['category']}', \" + (f\"'{v['job']}'\" if v.get('job') else 'NULL') + \", \" + (f\"'{v['characters']}'\" if v.get('characters') else 'NULL') + \")\"))\n",
    "                for k, v in test_principals.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_AKA_TITLE,\n",
    "            [\n",
    "                (INSERT_AKA_TITLE + str(k), lambda k=k, v=v: mysql_operation(mysql_client,\n",
    "                    f\"INSERT INTO aka_titles (title_id, ordering, aka_title, region, language, types, attributes, is_original_title) VALUES ('{v['title_id']}', {v['ordering']}, '{v['aka_title']}', \" + (f\"'{v['region']}'\" if v.get('region') else 'NULL') + \", \" + (f\"'{v['language']}'\" if v.get('language') else 'NULL') + \", \" + (f\"'{v['types']}'\" if v.get('types') else 'NULL') + \", \" + (f\"'{v['attributes']}'\" if v.get('attributes') else 'NULL') + f\", {v['is_original_title']})\"))\n",
    "                for k, v in test_aka_titles.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_EPISODE,\n",
    "            [\n",
    "                (INSERT_EPISODE + str(k), lambda k=k, v=v: mysql_operation(mysql_client,\n",
    "                    f\"INSERT INTO episodes (episode_id, parent_id, season_number, episode_number) VALUES ('{k}', '{v['parent_id']}', \" +\n",
    "                    (f\"{v['season_number']}\" if v.get('season_number') is not None else 'NULL') + \", \" +\n",
    "                    (f\"{v['episode_number']}\" if v.get('episode_number') is not None else 'NULL') +\n",
    "                    \")\"))\n",
    "                for k, v in test_episodes.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_TITLE_GENRE,\n",
    "            [\n",
    "                (INSERT_TITLE_GENRE + str(k), lambda k=k, v=v: mysql_operation(mysql_client,\n",
    "                    f\"INSERT INTO title_genres (title_id, genre) VALUES ('{v['title_id']}', '{v['genre']}')\"))\n",
    "                for k, v in test_title_genres.items()\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    select_scenarios = [\n",
    "        (\n",
    "            SELECT_TITLE, [\n",
    "                (SELECT_TITLE + str(k), lambda k=k: mysql_operation(mysql_client,\n",
    "                    f\"SELECT * FROM titles WHERE tconst = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_PERSON, [\n",
    "                (SELECT_PERSON + str(k), lambda k=k: mysql_operation(mysql_client,\n",
    "                    f\"SELECT * FROM people WHERE nconst = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_PEOPLE_IN_TITLE, [\n",
    "                (SELECT_ALL_PEOPLE_IN_TITLE + str(k), lambda k=k: mysql_operation(mysql_client,\n",
    "                    f\"SELECT * FROM people p JOIN principals pr ON p.nconst = pr.person_id WHERE pr.title_id = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_EPISODES_FOR_SERIES, [\n",
    "                (SELECT_ALL_EPISODES_FOR_SERIES + str(k), lambda k=k: mysql_operation(mysql_client,\n",
    "                    f\"SELECT * FROM episodes e JOIN titles t ON e.episode_id = t.tconst WHERE e.parent_id = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ] \n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_RATINGS_WITH_TITLE_INFO, [\n",
    "                (SELECT_ALL_RATINGS_WITH_TITLE_INFO + str(k), lambda k=k: mysql_operation(mysql_client,\n",
    "                    f\"SELECT * FROM ratings r JOIN titles t ON r.title_id = t.tconst JOIN title_genres tg ON t.tconst = tg.title_id WHERE tg.genre = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ] \n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    update_scenarios = [\n",
    "        (\n",
    "            UPDATE_TITLE_PRIMARY_TITLE, [\n",
    "                (UPDATE_TITLE_PRIMARY_TITLE + str(k), lambda k=k: mysql_operation(mysql_client,\n",
    "                    f\"UPDATE titles SET primary_title = 'UPDATED' WHERE tconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_ALL_RATINGS_FOR_TITLE, [\n",
    "                (UPDATE_ALL_RATINGS_FOR_TITLE + str(k), lambda k=k: mysql_operation(mysql_client,\n",
    "                    f\"UPDATE ratings SET average_rating = 10.0 WHERE title_id = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_PERSON_PRIMARY_NAME, [\n",
    "                (UPDATE_PERSON_PRIMARY_NAME + str(k), lambda k=k: mysql_operation(mysql_client,\n",
    "                    f\"UPDATE people SET primary_name = 'UPDATED' WHERE nconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_TITLE_START_YEAR, [\n",
    "                (UPDATE_TITLE_START_YEAR + str(k), lambda k=k: mysql_operation(mysql_client,\n",
    "                    f\"UPDATE titles SET start_year = 2024 WHERE tconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_PERSON_BIRTH_YEAR_FOR_ALL_PEOPLE_IN_TITLE, [\n",
    "                (UPDATE_PERSON_BIRTH_YEAR_FOR_ALL_PEOPLE_IN_TITLE + str(k), lambda k=k: mysql_operation(mysql_client,\n",
    "                    f\"UPDATE people SET birth_year = 2000 WHERE nconst IN (SELECT person_id FROM principals WHERE title_id = '{k}')\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    delete_scenarios = [\n",
    "        (\n",
    "            DELETE_TITLE, [\n",
    "                (DELETE_TITLE + str(k), lambda k=k: mysql_operation(mysql_client,\n",
    "                    f\"DELETE FROM titles WHERE tconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_PERSON, [\n",
    "                (DELETE_PERSON + str(k), lambda k=k: mysql_operation(mysql_client,\n",
    "                    f\"DELETE FROM people WHERE nconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_GENRES_THAT_ARE_IN_THE_TITLE, [\n",
    "                (DELETE_GENRES_THAT_ARE_IN_THE_TITLE + str(k), lambda k=k: mysql_operation(mysql_client,\n",
    "                    f\"DELETE FROM title_genres WHERE title_id = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_PEOPLE_WHO_ARE_IN_TITLE, [\n",
    "                (DELETE_PEOPLE_WHO_ARE_IN_TITLE + str(k), lambda k=k: mysql_operation(mysql_client,\n",
    "                    f\"DELETE FROM principals WHERE title_id = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    benchmark.run_benchmark(insert_scenarios)\n",
    "    #benchmark.run_benchmark(insert_scenarios, setup_method=setup_for_insert) # TEST do testow bez setup_method\n",
    "    benchmark.run_benchmark(select_scenarios)\n",
    "    benchmark.run_benchmark(update_scenarios)\n",
    "    benchmark.run_benchmark(delete_scenarios)\n",
    "    return benchmark.get_results_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b35b73e2-63e6-4d11-9281-5e470e4aa598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_int_or_none(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    # już jest int, zostaw\n",
    "    if isinstance(v, int):\n",
    "        return v\n",
    "    s = str(v)\n",
    "    if s == \"\":\n",
    "        return None\n",
    "    return int(s)\n",
    "\n",
    "def _cassandra_select_ratings_for_genre(session, select_title_ids_stmt, select_rating_by_title_stmt, genre: str):\n",
    "    \"\"\"\n",
    "    Zastępnik dla złożonego JOIN-a SQL w Cassandra.\n",
    "    \"\"\"\n",
    "    title_rows = session.execute(select_title_ids_stmt, (genre,))\n",
    "    title_ids = [row.title_id for row in title_rows]\n",
    "\n",
    "    if not title_ids:\n",
    "        return []\n",
    "\n",
    "    ratings = []\n",
    "    for tid in title_ids:\n",
    "        rows = session.execute(select_rating_by_title_stmt, (tid,))\n",
    "        ratings.extend(list(rows))\n",
    "\n",
    "    return ratings\n",
    "\n",
    "def _cassandra_update_people_birth_year_for_title(\n",
    "    session,\n",
    "    select_principals_by_title_stmt,\n",
    "    update_person_birth_year_stmt,\n",
    "    title_id: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Cassandra odpowiednik:\n",
    "    UPDATE people\n",
    "    SET birth_year = 2000\n",
    "    WHERE nconst IN (SELECT person_id FROM principals WHERE title_id = ?)\n",
    "    \"\"\"\n",
    "    rows = session.execute(select_principals_by_title_stmt, (title_id,))\n",
    "    person_ids = {row.person_id for row in rows}\n",
    "\n",
    "    for pid in person_ids:\n",
    "        session.execute(update_person_birth_year_stmt, (pid,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c8a2eee8-287b-477b-bcf9-a84fb8eaf8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cassandra_benchmark(all_data_dir, data_dir, random_numbers_list) -> pd.DataFrame:\n",
    "    benchmark = BenchmarkTime(\"cassandra\", data_dir)\n",
    "\n",
    "    def setup_for_insert():\n",
    "        with open('db/cassandra/schema.cql', 'r') as f:\n",
    "            cassandra_schema = f.read()\n",
    "\n",
    "        initialize_cassandra_schema(cassandra_session, cassandra_schema)\n",
    "\n",
    "        expected_tables = ['titles', 'aka_titles', 'ratings',\n",
    "                           'people', 'principals', 'episodes', 'title_genres']\n",
    "        verify_cassandra_tables(cassandra_session, expected_tables)\n",
    "\n",
    "        load_cassandra_data(cassandra_session, all_data_dir)\n",
    "\n",
    "    # dane do pojedynczych INSERT-ów (tak jak w postgres_benchmark)\n",
    "    test_data = load_insert_data_from_tsv(data_dir)\n",
    "    test_titles = test_data['titles']\n",
    "    test_people = test_data['people']\n",
    "    test_ratings = test_data['ratings']\n",
    "    test_principals = test_data['principals']\n",
    "    test_aka_titles = test_data['aka_titles']\n",
    "    test_episodes = test_data['episodes']\n",
    "    test_title_genres = test_data['title_genres']\n",
    "\n",
    "    # --------- prepared statements dla INSERT-ów ---------\n",
    "    insert_title_stmt = cassandra_session.prepare(\"\"\"\n",
    "        INSERT INTO imdb.titles (\n",
    "            tconst, title_type, primary_title, original_title,\n",
    "            is_adult, start_year, end_year, runtime_minutes\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "\n",
    "    insert_person_stmt = cassandra_session.prepare(\"\"\"\n",
    "        INSERT INTO imdb.people (\n",
    "            nconst, primary_name, birth_year, death_year, primary_profession\n",
    "        ) VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "\n",
    "    insert_rating_stmt = cassandra_session.prepare(\"\"\"\n",
    "        INSERT INTO imdb.ratings (\n",
    "            title_id, average_rating, num_votes\n",
    "        ) VALUES (?, ?, ?)\n",
    "    \"\"\")\n",
    "\n",
    "    insert_principal_stmt = cassandra_session.prepare(\"\"\"\n",
    "        INSERT INTO imdb.principals (\n",
    "            title_id, ordering, person_id, category, job, characters\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "\n",
    "    insert_aka_title_stmt = cassandra_session.prepare(\"\"\"\n",
    "        INSERT INTO imdb.aka_titles (\n",
    "            title_id, ordering, aka_title, region, language,\n",
    "            types, attributes, is_original_title\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "\n",
    "    insert_episode_stmt = cassandra_session.prepare(\"\"\"\n",
    "        INSERT INTO imdb.episodes (\n",
    "            episode_id, parent_id, season_number, episode_number\n",
    "        ) VALUES (?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "\n",
    "    insert_title_genre_stmt = cassandra_session.prepare(\"\"\"\n",
    "        INSERT INTO imdb.title_genres (\n",
    "            title_id, genre\n",
    "        ) VALUES (?, ?)\n",
    "    \"\"\")\n",
    "\n",
    "    # --------- scenariusze INSERT ---------\n",
    "    insert_scenarios = [\n",
    "        (\n",
    "            INSERT_TITLE,\n",
    "            [\n",
    "                (\n",
    "                    INSERT_TITLE + str(k),\n",
    "                    lambda k=k, v=v: cassandra_session.execute(\n",
    "                        insert_title_stmt,\n",
    "                        (\n",
    "                            k,\n",
    "                            v['title_type'],\n",
    "                            v['primary_title'],\n",
    "                            v['original_title'],\n",
    "                            bool(v['is_adult']),\n",
    "                            _to_int_or_none(v.get('start_year')),\n",
    "                            _to_int_or_none(v.get('end_year')),\n",
    "                            _to_int_or_none(v.get('runtime_minutes')),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                )\n",
    "                for k, v in test_titles.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_PERSON,\n",
    "            [\n",
    "                (\n",
    "                    INSERT_PERSON + str(k),\n",
    "                    lambda k=k, v=v: cassandra_session.execute(\n",
    "                        insert_person_stmt,\n",
    "                        (\n",
    "                            k,\n",
    "                            v['primary_name'],\n",
    "                            _to_int_or_none(v.get('birth_year')),\n",
    "                            _to_int_or_none(v.get('death_year')),\n",
    "                            v.get('primary_profession'),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                for k, v in test_people.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_RATING,\n",
    "            [\n",
    "                (\n",
    "                    INSERT_RATING + str(k),\n",
    "                    lambda k=k, v=v: cassandra_session.execute(\n",
    "                        insert_rating_stmt,\n",
    "                        (\n",
    "                            k,\n",
    "                            float(v['average_rating']),\n",
    "                            _to_int_or_none(v.get('num_votes')),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                for k, v in test_ratings.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_PRINCIPAL,\n",
    "            [\n",
    "                (\n",
    "                    INSERT_PRINCIPAL + str(k),\n",
    "                    lambda k=k, v=v: cassandra_session.execute(\n",
    "                        insert_principal_stmt,\n",
    "                        (\n",
    "                            v['title_id'],\n",
    "                            _to_int_or_none(v.get('ordering')),\n",
    "                            v['person_id'],\n",
    "                            v['category'],\n",
    "                            v.get('job'),\n",
    "                            v.get('characters'),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                for k, v in test_principals.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_AKA_TITLE,\n",
    "            [\n",
    "                (\n",
    "                    INSERT_AKA_TITLE + str(k),\n",
    "                    lambda k=k, v=v: cassandra_session.execute(\n",
    "                        insert_aka_title_stmt,\n",
    "                        (\n",
    "                            v['title_id'],\n",
    "                            _to_int_or_none(v.get('ordering')),\n",
    "                            v['aka_title'],\n",
    "                            v.get('region'),\n",
    "                            v.get('language'),\n",
    "                            v.get('types'),\n",
    "                            v.get('attributes'),\n",
    "                            bool(v['is_original_title']),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                for k, v in test_aka_titles.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_EPISODE,\n",
    "            [\n",
    "                (\n",
    "                    INSERT_EPISODE + str(k),\n",
    "                    lambda k=k, v=v: cassandra_session.execute(\n",
    "                        insert_episode_stmt,\n",
    "                        (\n",
    "                            k,\n",
    "                            v['parent_id'],\n",
    "                            _to_int_or_none(v.get('season_number')),\n",
    "                            _to_int_or_none(v.get('episode_number')),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                for k, v in test_episodes.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_TITLE_GENRE,\n",
    "            [\n",
    "                (\n",
    "                    INSERT_TITLE_GENRE + str(k),\n",
    "                    lambda k=k, v=v: cassandra_session.execute(\n",
    "                        insert_title_genre_stmt,\n",
    "                        (\n",
    "                            v['title_id'],\n",
    "                            v['genre'],\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                for k, v in test_title_genres.items()\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # --------- prepared statements dla SELECT-ów ---------\n",
    "    select_title_stmt = cassandra_session.prepare(\"\"\"\n",
    "        SELECT * FROM imdb.titles WHERE tconst = ?\n",
    "    \"\"\")\n",
    "\n",
    "    select_person_stmt = cassandra_session.prepare(\"\"\"\n",
    "        SELECT * FROM imdb.people WHERE nconst = ?\n",
    "    \"\"\")\n",
    "\n",
    "    # tutaj zamiast joina z people robimy SELECT z principals\n",
    "    select_principals_by_title_stmt = cassandra_session.prepare(\"\"\"\n",
    "        SELECT * FROM imdb.principals WHERE title_id = ?\n",
    "    \"\"\")\n",
    "\n",
    "    # parent_id nie jest kluczem – trzeba ALLOW FILTERING (OK na benchmark)\n",
    "    select_episodes_by_parent_stmt = cassandra_session.prepare(\"\"\"\n",
    "        SELECT * FROM imdb.episodes WHERE parent_id = ? ALLOW FILTERING\n",
    "    \"\"\")\n",
    "\n",
    "    # wyciągamy title_id po genre, reszta w Pythonie\n",
    "    select_title_ids_by_genre_stmt = cassandra_session.prepare(\"\"\"\n",
    "        SELECT title_id FROM imdb.title_genres WHERE genre = ? ALLOW FILTERING\n",
    "    \"\"\")\n",
    "\n",
    "    select_rating_by_title_stmt = cassandra_session.prepare(\"\"\"\n",
    "        SELECT * FROM imdb.ratings WHERE title_id = ?\n",
    "    \"\"\")\n",
    "    \n",
    "    select_scenarios = [\n",
    "    (\n",
    "        SELECT_TITLE,\n",
    "        [\n",
    "            (\n",
    "                SELECT_TITLE + str(k),\n",
    "                lambda k=k: list(\n",
    "                    cassandra_session.execute(\n",
    "                        select_title_stmt,\n",
    "                        (str(k),)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            for k in random_numbers_list\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        SELECT_PERSON,\n",
    "        [\n",
    "            (\n",
    "                SELECT_PERSON + str(k),\n",
    "                lambda k=k: list(\n",
    "                    cassandra_session.execute(\n",
    "                        select_person_stmt,\n",
    "                        (str(k),)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            for k in random_numbers_list\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        SELECT_ALL_PEOPLE_IN_TITLE,\n",
    "        [\n",
    "            # uproszczenie: bierzemy wiersze z principals dla danego title_id\n",
    "            (\n",
    "                SELECT_ALL_PEOPLE_IN_TITLE + str(k),\n",
    "                lambda k=k: list(\n",
    "                    cassandra_session.execute(\n",
    "                        select_principals_by_title_stmt,\n",
    "                        (str(k),)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            for k in random_numbers_list\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        SELECT_ALL_EPISODES_FOR_SERIES,\n",
    "        [\n",
    "            (\n",
    "                SELECT_ALL_EPISODES_FOR_SERIES + str(k),\n",
    "                lambda k=k: list(\n",
    "                    cassandra_session.execute(\n",
    "                        select_episodes_by_parent_stmt,\n",
    "                        (str(k),)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            for k in random_numbers_list\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        SELECT_ALL_RATINGS_WITH_TITLE_INFO,\n",
    "        [\n",
    "            (\n",
    "                SELECT_ALL_RATINGS_WITH_TITLE_INFO + str(k),\n",
    "                lambda k=k: _cassandra_select_ratings_for_genre(\n",
    "                    cassandra_session,\n",
    "                    select_title_ids_by_genre_stmt,\n",
    "                    select_rating_by_title_stmt,\n",
    "                    str(k),\n",
    "                )\n",
    "            )\n",
    "            for k in random_numbers_list\n",
    "        ]\n",
    "    ),\n",
    "    ]\n",
    "\n",
    "        # --------- prepared statements dla UPDATE-ów ---------\n",
    "    update_title_primary_title_stmt = cassandra_session.prepare(\"\"\"\n",
    "        UPDATE imdb.titles\n",
    "        SET primary_title = 'UPDATED'\n",
    "        WHERE tconst = ?\n",
    "    \"\"\")\n",
    "\n",
    "    update_rating_for_title_stmt = cassandra_session.prepare(\"\"\"\n",
    "        UPDATE imdb.ratings\n",
    "        SET average_rating = 10.0\n",
    "        WHERE title_id = ?\n",
    "    \"\"\")\n",
    "\n",
    "    update_person_primary_name_stmt = cassandra_session.prepare(\"\"\"\n",
    "        UPDATE imdb.people\n",
    "        SET primary_name = 'UPDATED'\n",
    "        WHERE nconst = ?\n",
    "    \"\"\")\n",
    "\n",
    "    update_title_start_year_stmt = cassandra_session.prepare(\"\"\"\n",
    "        UPDATE imdb.titles\n",
    "        SET start_year = 2024\n",
    "        WHERE tconst = ?\n",
    "    \"\"\")\n",
    "\n",
    "    update_person_birth_year_stmt = cassandra_session.prepare(\"\"\"\n",
    "        UPDATE imdb.people\n",
    "        SET birth_year = 2000\n",
    "        WHERE nconst = ?\n",
    "    \"\"\")\n",
    "\n",
    "    \n",
    "    update_scenarios = [\n",
    "        (\n",
    "            UPDATE_TITLE_PRIMARY_TITLE,\n",
    "            [\n",
    "                (\n",
    "                    UPDATE_TITLE_PRIMARY_TITLE + str(k),\n",
    "                    lambda k=k: cassandra_session.execute(\n",
    "                        update_title_primary_title_stmt,\n",
    "                        (str(k),)\n",
    "                    )\n",
    "                )\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_ALL_RATINGS_FOR_TITLE,\n",
    "            [\n",
    "                (\n",
    "                    UPDATE_ALL_RATINGS_FOR_TITLE + str(k),\n",
    "                    lambda k=k: cassandra_session.execute(\n",
    "                        update_rating_for_title_stmt,\n",
    "                        (str(k),)\n",
    "                    )\n",
    "                )\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_PERSON_PRIMARY_NAME,\n",
    "            [\n",
    "                (\n",
    "                    UPDATE_PERSON_PRIMARY_NAME + str(k),\n",
    "                    lambda k=k: cassandra_session.execute(\n",
    "                        update_person_primary_name_stmt,\n",
    "                        (str(k),)\n",
    "                    )\n",
    "                )\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_TITLE_START_YEAR,\n",
    "            [\n",
    "                (\n",
    "                    UPDATE_TITLE_START_YEAR + str(k),\n",
    "                    lambda k=k: cassandra_session.execute(\n",
    "                        update_title_start_year_stmt,\n",
    "                        (str(k),)\n",
    "                    )\n",
    "                )\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_PERSON_BIRTH_YEAR_FOR_ALL_PEOPLE_IN_TITLE,\n",
    "            [\n",
    "                (\n",
    "                    UPDATE_PERSON_BIRTH_YEAR_FOR_ALL_PEOPLE_IN_TITLE + str(k),\n",
    "                    lambda k=k: _cassandra_update_people_birth_year_for_title(\n",
    "                        cassandra_session,\n",
    "                        select_principals_by_title_stmt,\n",
    "                        update_person_birth_year_stmt,\n",
    "                        str(k),\n",
    "                    )\n",
    "                )\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # --------- prepared statements dla DELETE-ów ---------\n",
    "    delete_title_stmt = cassandra_session.prepare(\"\"\"\n",
    "        DELETE FROM imdb.titles WHERE tconst = ?\n",
    "    \"\"\")\n",
    "\n",
    "    delete_person_stmt = cassandra_session.prepare(\"\"\"\n",
    "        DELETE FROM imdb.people WHERE nconst = ?\n",
    "    \"\"\")\n",
    "\n",
    "    delete_title_genres_by_title_stmt = cassandra_session.prepare(\"\"\"\n",
    "        DELETE FROM imdb.title_genres WHERE title_id = ?\n",
    "    \"\"\")\n",
    "\n",
    "    delete_principals_by_title_stmt = cassandra_session.prepare(\"\"\"\n",
    "        DELETE FROM imdb.principals WHERE title_id = ?\n",
    "    \"\"\")\n",
    "\n",
    "    delete_scenarios = [\n",
    "        (\n",
    "            DELETE_TITLE,\n",
    "            [\n",
    "                (\n",
    "                    DELETE_TITLE + str(k),\n",
    "                    lambda k=k: cassandra_session.execute(\n",
    "                        delete_title_stmt,\n",
    "                        (str(k),)\n",
    "                    )\n",
    "                )\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_PERSON,\n",
    "            [\n",
    "                (\n",
    "                    DELETE_PERSON + str(k),\n",
    "                    lambda k=k: cassandra_session.execute(\n",
    "                        delete_person_stmt,\n",
    "                        (str(k),)\n",
    "                    )\n",
    "                )\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_GENRES_THAT_ARE_IN_THE_TITLE,\n",
    "            [\n",
    "                (\n",
    "                    DELETE_GENRES_THAT_ARE_IN_THE_TITLE + str(k),\n",
    "                    lambda k=k: cassandra_session.execute(\n",
    "                        delete_title_genres_by_title_stmt,\n",
    "                        (str(k),)\n",
    "                    )\n",
    "                )\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_PEOPLE_WHO_ARE_IN_TITLE,\n",
    "            [\n",
    "                (\n",
    "                    DELETE_PEOPLE_WHO_ARE_IN_TITLE + str(k),\n",
    "                    lambda k=k: cassandra_session.execute(\n",
    "                        delete_principals_by_title_stmt,\n",
    "                        (str(k),)\n",
    "                    )\n",
    "                )\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "    benchmark.run_benchmark(insert_scenarios)\n",
    "    #benchmark.run_benchmark(insert_scenarios, setup_method=setup_for_insert) # TEST do testow bez setup_method\n",
    "    benchmark.run_benchmark(select_scenarios)\n",
    "    benchmark.run_benchmark(update_scenarios)\n",
    "    benchmark.run_benchmark(delete_scenarios)\n",
    "    return benchmark.get_results_df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b7516f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG=True\n",
    "from random import sample\n",
    "def run_all_benchmarks(number_of_operations):\n",
    "    data_path = f\"./data/processed_{number_of_operations}\"\n",
    "\n",
    "    #rand_list = sample(range(1, 10000000), number_of_operations)\n",
    "    rand_list = sample(range(1, 37), 30) # TEST to do testow ograniczenie danych\n",
    "\n",
    "    results = []\n",
    "    if DEBUG:\n",
    "        print(\"Running PostgreSQL benchmark...\")\n",
    "    #postgres_results_df = postgres_benchmark(\"./data/processed\", data_path, rand_list)\n",
    "    postgres_results_df = postgres_benchmark(\"./data/processed_100\", data_path, rand_list) # TEST to do testow ograniczenie danych\n",
    "    results.append(postgres_results_df)\n",
    "\n",
    "    if DEBUG:\n",
    "        print(\"Running MySQL benchmark...\")\n",
    "    #mysql_results_df = mysql_benchmark(\"./data/processed\", data_path, rand_list)\n",
    "    mysql_results_df = mysql_benchmark(\"./data/processed_100\", data_path, rand_list) # TEST to do testow ograniczenie danych\n",
    "    results.append(mysql_results_df)\n",
    "\n",
    "    if DEBUG:\n",
    "        print(\"Running Cassandra benchmark...\")\n",
    "    #cassandra_results_df = cassandra_benchmark(\"./data/processed\", data_path, rand_list)\n",
    "    cassandra_results_df = cassandra_benchmark(\"./data/processed_100\", data_path, rand_list) # TEST to do testow ograniczenie danych\n",
    "    results.append(cassandra_results_df)\n",
    "\n",
    "    merged_df = pd.concat(results, ignore_index=True)\n",
    "    merged_df.to_csv(f\"{data_path}/merged_results.csv\", index=False)\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(f\"Saved merged results to {data_path}/merged_results.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8c3dde58-601c-4fcf-a899-51714c44f5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PostgreSQL benchmark...\n",
      "--- INSERT Title ? (postgres) ---\n",
      "Total time: 0.7514 seconds\n",
      "Operations: 234\n",
      "Avg operation time: 0.0032 seconds\n",
      "Throughput: 311.40 ops/sec\n",
      "CPU avg: 0.89%\n",
      "Memory avg: 162.52 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 3.04 MB\n",
      "\n",
      "--- INSERT Person ? (postgres) ---\n",
      "Total time: 0.1209 seconds\n",
      "Operations: 39\n",
      "Avg operation time: 0.0029 seconds\n",
      "Throughput: 322.46 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.53 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.49 MB\n",
      "\n",
      "--- INSERT Rating ? (postgres) ---\n",
      "Total time: 0.3140 seconds\n",
      "Operations: 100\n",
      "Avg operation time: 0.0031 seconds\n",
      "Throughput: 318.44 ops/sec\n",
      "CPU avg: 1.95%\n",
      "Memory avg: 162.53 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 1.43 MB\n",
      "\n",
      "--- INSERT Principal ? (postgres) ---\n",
      "Total time: 0.3257 seconds\n",
      "Operations: 100\n",
      "Avg operation time: 0.0031 seconds\n",
      "Throughput: 307.00 ops/sec\n",
      "CPU avg: 2.02%\n",
      "Memory avg: 162.53 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 1.38 MB\n",
      "\n",
      "--- INSERT AKA Title ? (postgres) ---\n",
      "Total time: 0.3158 seconds\n",
      "Operations: 100\n",
      "Avg operation time: 0.0031 seconds\n",
      "Throughput: 316.70 ops/sec\n",
      "CPU avg: 3.00%\n",
      "Memory avg: 162.53 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 1.32 MB\n",
      "\n",
      "--- INSERT Episode ? (postgres) ---\n",
      "Total time: 0.3049 seconds\n",
      "Operations: 100\n",
      "Avg operation time: 0.0030 seconds\n",
      "Throughput: 327.98 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.53 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 1.47 MB\n",
      "\n",
      "--- INSERT Title Genre ? (postgres) ---\n",
      "Total time: 0.3419 seconds\n",
      "Operations: 100\n",
      "Avg operation time: 0.0034 seconds\n",
      "Throughput: 292.46 ops/sec\n",
      "CPU avg: 1.95%\n",
      "Memory avg: 162.53 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 1.35 MB\n",
      "\n",
      "--- SELECT title ? (postgres) ---\n",
      "Total time: 0.0216 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0007 seconds\n",
      "Throughput: 1387.68 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.53 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- SELECT person ? (postgres) ---\n",
      "Total time: 0.0208 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0006 seconds\n",
      "Throughput: 1444.39 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.54 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- SELECT all people that are in the title ? (postgres) ---\n",
      "Total time: 0.0204 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0006 seconds\n",
      "Throughput: 1471.00 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.54 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- SELECT all episodes for the series ? (postgres) ---\n",
      "Total time: 0.0285 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0007 seconds\n",
      "Throughput: 1052.76 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.54 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- SELECT all ratings with title info for all titles in the genre ? (postgres) ---\n",
      "Total time: 0.0289 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0009 seconds\n",
      "Throughput: 1036.42 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.54 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- UPDATE Title ? Primary Title (postgres) ---\n",
      "Total time: 0.0545 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0015 seconds\n",
      "Throughput: 549.97 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.54 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- UPDATE all Ratings for Title ? (postgres) ---\n",
      "Total time: 0.0590 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0020 seconds\n",
      "Throughput: 508.33 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.54 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- UPDATE Person ? Primary Name (postgres) ---\n",
      "Total time: 0.0507 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0015 seconds\n",
      "Throughput: 591.22 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.54 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- UPDATE Title ? Start Year (postgres) ---\n",
      "Total time: 0.0524 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0017 seconds\n",
      "Throughput: 572.25 ops/sec\n",
      "CPU avg: 3.26%\n",
      "Memory avg: 162.54 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- UPDATE Person Birth Year for all people that are in the title ? (postgres) ---\n",
      "Total time: 0.0574 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0019 seconds\n",
      "Throughput: 522.21 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.54 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- DELETE Title ? (postgres) ---\n",
      "Total time: 0.0539 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0017 seconds\n",
      "Throughput: 556.86 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.54 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- DELETE Person ? (postgres) ---\n",
      "Total time: 0.0600 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0019 seconds\n",
      "Throughput: 500.30 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.54 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- DELETE Genres that are in the title ? (postgres) ---\n",
      "Total time: 0.0550 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0018 seconds\n",
      "Throughput: 545.47 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.54 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- DELETE People who are in title ? (postgres) ---\n",
      "Total time: 0.0501 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0017 seconds\n",
      "Throughput: 599.00 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.54 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "Running MySQL benchmark...\n",
      "--- INSERT Title ? (mysql) ---\n",
      "Total time: 1.6567 seconds\n",
      "Operations: 234\n",
      "Avg operation time: 0.0069 seconds\n",
      "Throughput: 141.25 ops/sec\n",
      "CPU avg: 3.84%\n",
      "Memory avg: 162.57 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 14.19 MB\n",
      "\n",
      "--- INSERT Person ? (mysql) ---\n",
      "Total time: 0.2268 seconds\n",
      "Operations: 39\n",
      "Avg operation time: 0.0058 seconds\n",
      "Throughput: 171.99 ops/sec\n",
      "CPU avg: 2.67%\n",
      "Memory avg: 162.57 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 2.00 MB\n",
      "\n",
      "--- INSERT Rating ? (mysql) ---\n",
      "Total time: 0.6482 seconds\n",
      "Operations: 100\n",
      "Avg operation time: 0.0065 seconds\n",
      "Throughput: 154.27 ops/sec\n",
      "CPU avg: 1.95%\n",
      "Memory avg: 162.57 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 5.32 MB\n",
      "\n",
      "--- INSERT Principal ? (mysql) ---\n",
      "Total time: 0.6798 seconds\n",
      "Operations: 100\n",
      "Avg operation time: 0.0067 seconds\n",
      "Throughput: 147.11 ops/sec\n",
      "CPU avg: 3.91%\n",
      "Memory avg: 162.57 MB\n",
      "Disk read: 0.04 MB\n",
      "Disk write: 5.50 MB\n",
      "\n",
      "--- INSERT AKA Title ? (mysql) ---\n",
      "Total time: 0.6594 seconds\n",
      "Operations: 100\n",
      "Avg operation time: 0.0066 seconds\n",
      "Throughput: 151.65 ops/sec\n",
      "CPU avg: 2.08%\n",
      "Memory avg: 162.57 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 5.43 MB\n",
      "\n",
      "--- INSERT Episode ? (mysql) ---\n",
      "Total time: 0.6453 seconds\n",
      "Operations: 100\n",
      "Avg operation time: 0.0064 seconds\n",
      "Throughput: 154.97 ops/sec\n",
      "CPU avg: 2.93%\n",
      "Memory avg: 162.57 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 5.38 MB\n",
      "\n",
      "--- INSERT Title Genre ? (mysql) ---\n",
      "Total time: 0.6782 seconds\n",
      "Operations: 100\n",
      "Avg operation time: 0.0067 seconds\n",
      "Throughput: 147.45 ops/sec\n",
      "CPU avg: 5.08%\n",
      "Memory avg: 162.57 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 5.24 MB\n",
      "\n",
      "--- SELECT title ? (mysql) ---\n",
      "Total time: 0.0452 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0013 seconds\n",
      "Throughput: 663.27 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.57 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.09 MB\n",
      "\n",
      "--- SELECT person ? (mysql) ---\n",
      "Total time: 0.0450 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0015 seconds\n",
      "Throughput: 667.05 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.57 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.04 MB\n",
      "\n",
      "--- SELECT all people that are in the title ? (mysql) ---\n",
      "Total time: 0.0470 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0015 seconds\n",
      "Throughput: 638.74 ops/sec\n",
      "CPU avg: 3.47%\n",
      "Memory avg: 162.57 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- SELECT all episodes for the series ? (mysql) ---\n",
      "Total time: 0.0431 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0013 seconds\n",
      "Throughput: 695.72 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.57 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- SELECT all ratings with title info for all titles in the genre ? (mysql) ---\n",
      "Total time: 0.0495 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0014 seconds\n",
      "Throughput: 605.99 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.58 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- UPDATE Title ? Primary Title (mysql) ---\n",
      "Total time: 0.0633 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0021 seconds\n",
      "Throughput: 473.94 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.58 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- UPDATE all Ratings for Title ? (mysql) ---\n",
      "Total time: 0.0650 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0021 seconds\n",
      "Throughput: 461.68 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.58 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.01 MB\n",
      "\n",
      "--- UPDATE Person ? Primary Name (mysql) ---\n",
      "Total time: 0.0569 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0019 seconds\n",
      "Throughput: 527.12 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.58 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.04 MB\n",
      "\n",
      "--- UPDATE Title ? Start Year (mysql) ---\n",
      "Total time: 0.0730 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0023 seconds\n",
      "Throughput: 410.78 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.58 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.01 MB\n",
      "\n",
      "--- UPDATE Person Birth Year for all people that are in the title ? (mysql) ---\n",
      "Total time: 0.0715 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0024 seconds\n",
      "Throughput: 419.70 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.58 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.01 MB\n",
      "\n",
      "--- DELETE Title ? (mysql) ---\n",
      "Total time: 0.0631 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0021 seconds\n",
      "Throughput: 475.74 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.58 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- DELETE Person ? (mysql) ---\n",
      "Total time: 0.0576 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0019 seconds\n",
      "Throughput: 520.94 ops/sec\n",
      "CPU avg: 3.26%\n",
      "Memory avg: 162.58 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.01 MB\n",
      "\n",
      "--- DELETE Genres that are in the title ? (mysql) ---\n",
      "Total time: 0.0749 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0025 seconds\n",
      "Throughput: 400.41 ops/sec\n",
      "CPU avg: 3.26%\n",
      "Memory avg: 162.58 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.01 MB\n",
      "\n",
      "--- DELETE People who are in title ? (mysql) ---\n",
      "Total time: 0.0558 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0017 seconds\n",
      "Throughput: 537.60 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 162.59 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "Running Cassandra benchmark...\n",
      "--- INSERT Title ? (cassandra) ---\n",
      "Total time: 3.0236 seconds\n",
      "Operations: 234\n",
      "Avg operation time: 0.0129 seconds\n",
      "Throughput: 77.39 ops/sec\n",
      "CPU avg: 4.50%\n",
      "Memory avg: 162.71 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 11.63 MB\n",
      "\n",
      "--- INSERT Person ? (cassandra) ---\n",
      "Total time: 0.5133 seconds\n",
      "Operations: 39\n",
      "Avg operation time: 0.0132 seconds\n",
      "Throughput: 75.98 ops/sec\n",
      "CPU avg: 2.51%\n",
      "Memory avg: 162.77 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.10 MB\n",
      "\n",
      "--- INSERT Rating ? (cassandra) ---\n",
      "Total time: 1.3615 seconds\n",
      "Operations: 100\n",
      "Avg operation time: 0.0136 seconds\n",
      "Throughput: 73.45 ops/sec\n",
      "CPU avg: 7.88%\n",
      "Memory avg: 162.78 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.13 MB\n",
      "\n",
      "--- INSERT Principal ? (cassandra) ---\n",
      "Total time: 1.3647 seconds\n",
      "Operations: 100\n",
      "Avg operation time: 0.0136 seconds\n",
      "Throughput: 73.27 ops/sec\n",
      "CPU avg: 4.04%\n",
      "Memory avg: 162.91 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.10 MB\n",
      "\n",
      "--- INSERT AKA Title ? (cassandra) ---\n",
      "Total time: 1.3415 seconds\n",
      "Operations: 100\n",
      "Avg operation time: 0.0132 seconds\n",
      "Throughput: 74.55 ops/sec\n",
      "CPU avg: 3.00%\n",
      "Memory avg: 163.01 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.03 MB\n",
      "\n",
      "--- INSERT Episode ? (cassandra) ---\n",
      "Total time: 1.3197 seconds\n",
      "Operations: 100\n",
      "Avg operation time: 0.0132 seconds\n",
      "Throughput: 75.78 ops/sec\n",
      "CPU avg: 1.99%\n",
      "Memory avg: 163.05 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.34 MB\n",
      "\n",
      "--- INSERT Title Genre ? (cassandra) ---\n",
      "Total time: 1.3947 seconds\n",
      "Operations: 100\n",
      "Avg operation time: 0.0139 seconds\n",
      "Throughput: 71.70 ops/sec\n",
      "CPU avg: 3.99%\n",
      "Memory avg: 163.05 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.48 MB\n",
      "\n",
      "--- SELECT title ? (cassandra) ---\n",
      "Total time: 0.3844 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0128 seconds\n",
      "Throughput: 78.04 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 163.05 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.04 MB\n",
      "\n",
      "--- SELECT person ? (cassandra) ---\n",
      "Total time: 0.4216 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0139 seconds\n",
      "Throughput: 71.16 ops/sec\n",
      "CPU avg: 3.47%\n",
      "Memory avg: 163.05 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- SELECT all people that are in the title ? (cassandra) ---\n",
      "Total time: 0.3774 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0126 seconds\n",
      "Throughput: 79.48 ops/sec\n",
      "CPU avg: 10.42%\n",
      "Memory avg: 163.06 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- SELECT all episodes for the series ? (cassandra) ---\n",
      "Total time: 0.4237 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0141 seconds\n",
      "Throughput: 70.81 ops/sec\n",
      "CPU avg: 6.51%\n",
      "Memory avg: 163.06 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.08 MB\n",
      "\n",
      "--- SELECT all ratings with title info for all titles in the genre ? (cassandra) ---\n",
      "Total time: 0.4109 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0137 seconds\n",
      "Throughput: 73.02 ops/sec\n",
      "CPU avg: 13.46%\n",
      "Memory avg: 163.06 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.02 MB\n",
      "\n",
      "--- UPDATE Title ? Primary Title (cassandra) ---\n",
      "Total time: 0.4237 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0141 seconds\n",
      "Throughput: 70.80 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 163.07 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- UPDATE all Ratings for Title ? (cassandra) ---\n",
      "Total time: 0.3782 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0126 seconds\n",
      "Throughput: 79.32 ops/sec\n",
      "CPU avg: 13.24%\n",
      "Memory avg: 163.07 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- UPDATE Person ? Primary Name (cassandra) ---\n",
      "Total time: 0.3989 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0132 seconds\n",
      "Throughput: 75.20 ops/sec\n",
      "CPU avg: 13.46%\n",
      "Memory avg: 163.07 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.01 MB\n",
      "\n",
      "--- UPDATE Title ? Start Year (cassandra) ---\n",
      "Total time: 0.3780 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0126 seconds\n",
      "Throughput: 79.36 ops/sec\n",
      "CPU avg: 9.99%\n",
      "Memory avg: 163.07 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- UPDATE Person Birth Year for all people that are in the title ? (cassandra) ---\n",
      "Total time: 0.4116 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0136 seconds\n",
      "Throughput: 72.89 ops/sec\n",
      "CPU avg: 9.99%\n",
      "Memory avg: 163.13 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- DELETE Title ? (cassandra) ---\n",
      "Total time: 0.4014 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0133 seconds\n",
      "Throughput: 74.74 ops/sec\n",
      "CPU avg: 3.47%\n",
      "Memory avg: 163.15 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.02 MB\n",
      "\n",
      "--- DELETE Person ? (cassandra) ---\n",
      "Total time: 0.3942 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0130 seconds\n",
      "Throughput: 76.11 ops/sec\n",
      "CPU avg: 23.01%\n",
      "Memory avg: 163.15 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "--- DELETE Genres that are in the title ? (cassandra) ---\n",
      "Total time: 0.4091 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0136 seconds\n",
      "Throughput: 73.33 ops/sec\n",
      "CPU avg: 3.26%\n",
      "Memory avg: 163.15 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.15 MB\n",
      "\n",
      "--- DELETE People who are in title ? (cassandra) ---\n",
      "Total time: 0.4002 seconds\n",
      "Operations: 30\n",
      "Avg operation time: 0.0133 seconds\n",
      "Throughput: 74.97 ops/sec\n",
      "CPU avg: 0.00%\n",
      "Memory avg: 163.15 MB\n",
      "Disk read: 0.00 MB\n",
      "Disk write: 0.00 MB\n",
      "\n",
      "Saved merged results to ./data/processed_100/merged_results.csv\n"
     ]
    }
   ],
   "source": [
    "run_all_benchmarks(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6fe6a148-9bd5-49c5-8c2f-948ee31cb18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titles -> 234\n",
      "people -> 39\n",
      "ratings -> 100\n",
      "principals -> 100\n",
      "aka_titles -> 100\n",
      "episodes -> 100\n",
      "title_genres -> 100\n"
     ]
    }
   ],
   "source": [
    "test_data = load_insert_data_from_tsv(data_dir=\"data\\processed_100\")\n",
    "\n",
    "for table, rows in test_data.items():\n",
    "    print(table, \"->\", len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1d077-b489-417a-8c91-3348ed5db160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
