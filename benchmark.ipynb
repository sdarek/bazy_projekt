{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb0bfa3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T18:25:33.541201Z",
     "start_time": "2025-11-26T18:25:33.361561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection has been established\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import psycopg2.errors\n",
    "from pymongo import MongoClient\n",
    "from cassandra.cluster import Cluster\n",
    "import mysql.connector\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "with open('docker-compose.yaml', 'r') as file:\n",
    "    docker_config = yaml.safe_load(file)\n",
    "\n",
    "postgres_config = docker_config['services']['postgres']\n",
    "postgres_client = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    database=postgres_config['environment']['POSTGRES_DB'],\n",
    "    user=postgres_config['environment']['POSTGRES_USER'],\n",
    "    password=postgres_config['environment']['POSTGRES_PASSWORD'],\n",
    "    port=postgres_config['ports'][0].split(':')[0]\n",
    ")\n",
    "\n",
    "mariadb_config = docker_config['services']['mariadb']\n",
    "mariadb_client = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    database=mariadb_config['environment']['MYSQL_DATABASE'],\n",
    "    user=mariadb_config['environment']['MYSQL_USER'],\n",
    "    password=mariadb_config['environment']['MYSQL_PASSWORD'],\n",
    "    port=mariadb_config['ports'][0].split(':')[0],\n",
    "    allow_local_infile=True\n",
    ")\n",
    "\n",
    "mysql_config = docker_config['services']['mysql']\n",
    "mysql_client = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    database=mysql_config['environment']['MYSQL_DATABASE'],\n",
    "    user=mysql_config['environment']['MYSQL_USER'],\n",
    "    password=mysql_config['environment']['MYSQL_PASSWORD'],\n",
    "    port=mysql_config['ports'][0].split(':')[0],\n",
    "    allow_local_infile=True\n",
    ")\n",
    "\n",
    "mongo_8_config = docker_config['services']['mongo-8']\n",
    "mongo_8_config = MongoClient(\n",
    "    host='localhost',\n",
    "    port=int(mongo_8_config['ports'][0].split(':')[0])\n",
    ")\n",
    "\n",
    "mongo_7_config = docker_config['services']['mongo-7']\n",
    "mongo_7_config = MongoClient(\n",
    "    host='localhost',\n",
    "    port=int(mongo_7_config['ports'][0].split(':')[0])\n",
    ")\n",
    "\n",
    "cassandra_config = docker_config['services']['cassandra']\n",
    "cassandra_client = Cluster(['localhost'], port=int(cassandra_config['ports'][0].split(':')[0]))\n",
    "cassandra_session = cassandra_client.connect()\n",
    "\n",
    "try:\n",
    "    postgres_client.cursor().execute(\"SELECT 0\")\n",
    "    mariadb_client.cursor(buffered=True).execute(\"SELECT 1\")\n",
    "    mysql_client.cursor(buffered=True).execute(\"SELECT 1\")\n",
    "    cassandra_session.execute(\"SELECT release_version FROM system.local\")\n",
    "    mongo_8_config.admin.command('ping')\n",
    "    mongo_7_config.admin.command('ping')\n",
    "    print(\"connection has been established\")\n",
    "except Exception as e:\n",
    "    print(\"connection test failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ae52e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_postgres_schema(conn, schema):\n",
    "    if not schema:\n",
    "        return\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(schema)\n",
    "        conn.commit()\n",
    "        print(\"PostgreSQL schema initialization complete.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(\"Error initializing PostgreSQL schema\")\n",
    "\n",
    "\n",
    "def verify_postgres_tables(conn, expected_tables):\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = 'public' AND table_name = ANY(%s);\n",
    "            \"\"\", (expected_tables,))\n",
    "            existing_tables = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "        missing_tables = set(expected_tables) - existing_tables\n",
    "        if not missing_tables:\n",
    "            print(f\"INFO: All PostgreSQL tables exist: {', '.join(expected_tables)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing PostgreSQL tables: {', '.join(missing_tables)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error verifying PostgreSQL tables: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de323680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL schema initialization complete.\n",
      "INFO: All PostgreSQL tables exist: titles, aka_titles, ratings, people, principals, episodes, title_genres\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('db/postgres/schema.sql', 'r') as f:\n",
    "    sql_schema = f.read()\n",
    "\n",
    "postgres_tables = ['titles', 'aka_titles', 'ratings', 'people', 'principals', 'episodes', 'title_genres']\n",
    "\n",
    "initialize_postgres_schema(postgres_client, sql_schema)\n",
    "verify_postgres_tables(postgres_client, postgres_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09843438-88e5-4fa5-8b35-9546856f428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "import re\n",
    "def preprocess_imdb(input_dir: str = \"./data\", output_dir: str = \"./data/processed\"):\n",
    "    \"\"\"\n",
    "    Przygotowuje pliki TSV zgodne z naszym ERD:\n",
    "    - titles.tsv\n",
    "    - people.tsv\n",
    "    - ratings.tsv\n",
    "    - principals.tsv\n",
    "    - aka_titles.tsv\n",
    "    - episodes.tsv\n",
    "    - title_genres.tsv\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    max_field_size = sys.maxsize\n",
    "    while True:\n",
    "        try:\n",
    "            csv.field_size_limit(max_field_size)\n",
    "            break\n",
    "        except OverflowError:\n",
    "            max_field_size = max_field_size // 10\n",
    "    in_path = Path(input_dir)\n",
    "    out_path = Path(output_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # regex: tab + \" + tekst bez \" i tabów, aż do taba albo końca linii\n",
    "    quote_fix = re.compile(r'\\t\"([^\"\\t]*)(?=\\t|$)')\n",
    "    # tekst do pierwszego taba + tab + \\N - dla people\n",
    "    people_fix = re.compile(r'^([^\\t]*\\t)\\\\N')\n",
    "\n",
    "    def cleaned_lines(f, clean_people=False):\n",
    "        for line in f:\n",
    "            # zamiana: \\t\"tekst   ->  \\ttekst\n",
    "            line = quote_fix.sub(r'\\t\\1', line)\n",
    "            \n",
    "            if clean_people:\n",
    "                line = people_fix.sub(r'\\1JakisTyp', line)\n",
    "                \n",
    "            yield line\n",
    "\n",
    "    valid_tconsts = set()\n",
    "    # ---------- T I T L E S  +  T I T L E _ G E N R E S ----------\n",
    "    basics_file = in_path / \"title.basics.tsv\"\n",
    "    if basics_file.exists():\n",
    "        titles_out = out_path / \"titles.tsv\"\n",
    "        genres_out = out_path / \"title_genres.tsv\"\n",
    "\n",
    "        with basics_file.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "             titles_out.open(\"w\", encoding=\"utf-8\", newline=\"\") as ftitles, \\\n",
    "             genres_out.open(\"w\", encoding=\"utf-8\", newline=\"\") as fgenres:\n",
    "                 \n",
    "\n",
    "            reader = csv.DictReader(cleaned_lines(fin), delimiter=\"\\t\")\n",
    "            wtitles = csv.writer(ftitles, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "            wgenres = csv.writer(fgenres, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "                 \n",
    "\n",
    "            # nagłówki zgodne ze schemą\n",
    "            wtitles.writerow([\n",
    "                \"tconst\",\n",
    "                \"title_type\",\n",
    "                \"primary_title\",\n",
    "                \"original_title\",\n",
    "                \"is_adult\",\n",
    "                \"start_year\",\n",
    "                \"end_year\",\n",
    "                \"runtime_minutes\",\n",
    "            ])\n",
    "            wgenres.writerow([\"title_id\", \"genre\"])\n",
    "\n",
    "            for row in reader:\n",
    "                tconst = row.get(\"tconst\", \"\\\\N\")\n",
    "                \n",
    "                if tconst and tconst != \"\\\\N\":\n",
    "                    valid_tconsts.add(tconst) \n",
    "\n",
    "                wtitles.writerow([\n",
    "                    tconst,\n",
    "                    row.get(\"titleType\", \"\\\\N\"),\n",
    "                    row.get(\"primaryTitle\", \"\\\\N\"),\n",
    "                    row.get(\"originalTitle\", \"\\\\N\"),\n",
    "                    row.get(\"isAdult\", \"\\\\N\"),\n",
    "                    row.get(\"startYear\", \"\\\\N\"),\n",
    "                    row.get(\"endYear\", \"\\\\N\"),\n",
    "                    row.get(\"runtimeMinutes\", \"\\\\N\"),\n",
    "                ])\n",
    "\n",
    "                genres = row.get(\"genres\", \"\\\\N\")\n",
    "                if genres and genres != \"\\\\N\":\n",
    "                    for g in genres.split(\",\"):\n",
    "                        g = g.strip()\n",
    "                        if g:\n",
    "                            wgenres.writerow([tconst, g])\n",
    "        print(\"[preprocess] titles + title_genres OK\")\n",
    "    else:\n",
    "        print(\"[preprocess] WARNING: title.basics.tsv not found\")\n",
    "\n",
    "    # ---------- P E O P L E ----------\n",
    "    name_file = in_path / \"name.basics.tsv\"\n",
    "    valid_nconsts = set()\n",
    "    if name_file.exists():\n",
    "        people_out = out_path / \"people.tsv\"\n",
    "        with name_file.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "             people_out.open(\"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "\n",
    "            reader = csv.DictReader(cleaned_lines(fin, clean_people=True), delimiter=\"\\t\")\n",
    "            w = csv.writer(fout, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "\n",
    "            # nconst, primary_name, birth_year, death_year, primary_profession\n",
    "            w.writerow([\"nconst\", \"primary_name\", \"birth_year\", \"death_year\", \"primary_profession\"])\n",
    "\n",
    "            for row in reader:\n",
    "                nconst = row.get(\"nconst\", \"\\\\N\")\n",
    "                if nconst and nconst != \"\\\\N\":\n",
    "                    valid_nconsts.add(nconst)\n",
    "                w.writerow([\n",
    "                    nconst,\n",
    "                    row.get(\"primaryName\", \"\\\\N\"),\n",
    "                    row.get(\"birthYear\", \"\\\\N\"),\n",
    "                    row.get(\"deathYear\", \"\\\\N\"),\n",
    "                    row.get(\"primaryProfession\", \"\\\\N\"),\n",
    "                ])\n",
    "        print(\"[preprocess] people OK\")\n",
    "    else:\n",
    "        print(\"[preprocess] WARNING: name.basics.tsv not found\")\n",
    "\n",
    "    # ---------- R A T I N G S ----------\n",
    "    ratings_file = in_path / \"title.ratings.tsv\"\n",
    "    if ratings_file.exists():\n",
    "        ratings_out = out_path / \"ratings.tsv\"\n",
    "        with ratings_file.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "             ratings_out.open(\"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "\n",
    "            reader = csv.DictReader(fin, delimiter=\"\\t\")\n",
    "            w = csv.writer(fout, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "\n",
    "            # tconst -> title_id\n",
    "            w.writerow([\"title_id\", \"average_rating\", \"num_votes\"])\n",
    "\n",
    "            for row in reader:\n",
    "                title_id = row.get(\"tconst\", \"\\\\N\")\n",
    "                if title_id in valid_tconsts:\n",
    "                    w.writerow([\n",
    "                        title_id,\n",
    "                        row.get(\"averageRating\", \"\\\\N\"),\n",
    "                        row.get(\"numVotes\", \"\\\\N\"),\n",
    "                    ])\n",
    "        print(\"[preprocess] ratings OK\")\n",
    "    else:\n",
    "        print(\"[preprocess] WARNING: title.ratings.tsv not found\")\n",
    "\n",
    "    # ---------- P R I N C I P A L S ----------\n",
    "    MAX_PRINCIPALS = 15000000  # Limit 15mln\n",
    "    count = 0\n",
    "    principals_file = in_path / \"title.principals.tsv\"\n",
    "    if principals_file.exists():\n",
    "        principals_out = out_path / \"principals.tsv\"\n",
    "        with principals_file.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "             principals_out.open(\"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "\n",
    "            reader = csv.DictReader(fin, delimiter=\"\\t\")\n",
    "            w = csv.writer(fout, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "\n",
    "            # title_id, ordering, person_id, category, job, characters\n",
    "            w.writerow([\"title_id\", \"ordering\", \"person_id\", \"category\", \"job\", \"characters\"])\n",
    "\n",
    "            for row in reader:\n",
    "                title_id = row.get(\"tconst\", \"\\\\N\")\n",
    "                person_id = row.get(\"nconst\", \"\\\\N\")\n",
    "                if person_id in valid_nconsts and title_id in valid_tconsts:\n",
    "                    if count >= MAX_PRINCIPALS:\n",
    "                        break\n",
    "                    w.writerow([\n",
    "                        title_id,\n",
    "                        row.get(\"ordering\", \"\\\\N\"),\n",
    "                        person_id,\n",
    "                        row.get(\"category\", \"\\\\N\"),\n",
    "                        row.get(\"job\", \"\\\\N\"),\n",
    "                        row.get(\"characters\", \"\\\\N\"),\n",
    "                    ])\n",
    "                    count += 1\n",
    "        print(\"[preprocess] principals OK\")\n",
    "    else:\n",
    "        print(\"[preprocess] WARNING: title.principals.tsv not found\")\n",
    "\n",
    "    # ---------- A K A _ T I T L E S ----------\n",
    "    akas_file = in_path / \"title.akas.tsv\"\n",
    "    if akas_file.exists():\n",
    "        akas_out = out_path / \"aka_titles.tsv\"\n",
    "        with akas_file.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "             akas_out.open(\"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "\n",
    "            reader = csv.DictReader(fin, delimiter=\"\\t\")\n",
    "            w = csv.writer(fout, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "\n",
    "            # title_id, ordering, aka_title, region, language, types, attributes, is_original_title\n",
    "            w.writerow([\n",
    "                \"title_id\", \"ordering\", \"aka_title\",\n",
    "                \"region\", \"language\", \"types\", \"attributes\", \"is_original_title\"\n",
    "            ])\n",
    "\n",
    "            for row in reader:\n",
    "                title_id = row.get(\"titleId\", \"\\\\N\")\n",
    "                if title_id in valid_tconsts:\n",
    "                    w.writerow([\n",
    "                        title_id,\n",
    "                        row.get(\"ordering\", \"\\\\N\"),\n",
    "                        row.get(\"title\", \"\\\\N\"),\n",
    "                        row.get(\"region\", \"\\\\N\"),\n",
    "                        row.get(\"language\", \"\\\\N\"),\n",
    "                        row.get(\"types\", \"\\\\N\"),\n",
    "                        row.get(\"attributes\", \"\\\\N\"),\n",
    "                        row.get(\"isOriginalTitle\", \"\\\\N\"),\n",
    "                    ])\n",
    "        print(\"[preprocess] aka_titles OK\")\n",
    "    else:\n",
    "        print(\"[preprocess] WARNING: title.akas.tsv not found\")\n",
    "\n",
    "    # ---------- E P I S O D E S ----------\n",
    "    episodes_file = in_path / \"title.episode.tsv\"\n",
    "    if episodes_file.exists():\n",
    "        episodes_out = out_path / \"episodes.tsv\"\n",
    "        with episodes_file.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "             episodes_out.open(\"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "\n",
    "            reader = csv.DictReader(fin, delimiter=\"\\t\")\n",
    "            w = csv.writer(fout, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "\n",
    "            # episode_id, parent_id, season_number, episode_number\n",
    "            w.writerow([\"episode_id\", \"parent_id\", \"season_number\", \"episode_number\"])\n",
    "\n",
    "            for row in reader:\n",
    "                episode_id = row.get(\"tconst\", \"\\\\N\") \n",
    "                parent_id = row.get(\"parentTconst\", \"\\\\N\") \n",
    "                if episode_id in valid_tconsts and parent_id in valid_tconsts:\n",
    "                    w.writerow([\n",
    "                        episode_id,\n",
    "                        parent_id,\n",
    "                        row.get(\"seasonNumber\", \"\\\\N\"),\n",
    "                        row.get(\"episodeNumber\", \"\\\\N\"),\n",
    "                    ])\n",
    "        print(\"[preprocess] episodes OK\")\n",
    "    else:\n",
    "        print(\"[preprocess] WARNING: title.episode.tsv not found\")\n",
    "\n",
    "    print(\"[preprocess] DONE. Pliki w:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bfbffdd-8633-4b0c-ba9a-4f555cc47ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preprocess] titles + title_genres OK\n",
      "[preprocess] people OK\n",
      "[preprocess] ratings OK\n",
      "[preprocess] principals OK\n",
      "[preprocess] aka_titles OK\n",
      "[preprocess] episodes OK\n",
      "[preprocess] DONE. Pliki w: data\\processed\n"
     ]
    }
   ],
   "source": [
    "preprocess_imdb(\"./data\", \"./data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9295428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_operation(conn, query, fetch=False):\n",
    "    \"\"\"\n",
    "    Execute a PostgreSQL query and optionally fetch results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    conn : psycopg2 connection\n",
    "        Database connection\n",
    "    query : str\n",
    "        SQL query to execute\n",
    "    fetch : bool\n",
    "        Whether to fetch and return results\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Result set if fetch=True, None otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            if fetch:\n",
    "                return cur.fetchall()\n",
    "        if not fetch:\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Błąd operacji PostgreSQL: {e}\")\n",
    "\n",
    "\n",
    "def load_insert_data_from_tsv(data_dir, max_rows=100):\n",
    "    \"\"\"\n",
    "    Load data from TSV files for INSERT benchmark operations.\n",
    "    Loads a subset of rows from each TSV file to use for INSERT benchmarks.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : str or Path\n",
    "        Directory containing TSV files\n",
    "    max_rows : int\n",
    "        Maximum number of rows to load from each file for INSERT operations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Dictionary containing test data for each table\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import csv\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    result = {\n",
    "        'titles': {},\n",
    "        'people': {},\n",
    "        'ratings': {},\n",
    "        'principals': {},\n",
    "        'aka_titles': {},\n",
    "        'episodes': {},\n",
    "        'title_genres': {}\n",
    "    }\n",
    "    \n",
    "    # Helper to escape SQL strings\n",
    "    def escape_sql(s):\n",
    "        if s is None or s == '\\\\N':\n",
    "            return None\n",
    "        return str(s).replace(\"'\", \"''\")\n",
    "    \n",
    "    # Load titles\n",
    "    titles_file = data_path / 'titles.tsv'\n",
    "    if titles_file.exists():\n",
    "        with open(titles_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                tconst = row.get('tconst', '')\n",
    "                if tconst:\n",
    "                    new_tconst = f\"NEW{tconst}\"\n",
    "                    result['titles'][new_tconst] = {\n",
    "                        'title_type': escape_sql(row.get('title_type', '')),\n",
    "                        'primary_title': escape_sql(row.get('primary_title', '')),\n",
    "                        'original_title': escape_sql(row.get('original_title', '')),\n",
    "                        'is_adult': row.get('is_adult', '0') == '1',\n",
    "                        'start_year': row.get('start_year', '') if row.get('start_year') != '\\\\N' else None,\n",
    "                        'end_year': row.get('end_year', '') if row.get('end_year') != '\\\\N' else None,\n",
    "                        'runtime_minutes': row.get('runtime_minutes', '') if row.get('runtime_minutes') != '\\\\N' else None\n",
    "                    }\n",
    "    \n",
    "    # Load people\n",
    "    people_file = data_path / 'people.tsv'\n",
    "    if people_file.exists():\n",
    "        with open(people_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                nconst = row.get('nconst', '')\n",
    "                if nconst:\n",
    "                    new_nconst = f\"NEW{nconst}\"\n",
    "                    result['people'][new_nconst] = {\n",
    "                        'primary_name': escape_sql(row.get('primary_name', '')),\n",
    "                        'birth_year': row.get('birth_year', '') if row.get('birth_year') != '\\\\N' else None,\n",
    "                        'death_year': row.get('death_year', '') if row.get('death_year') != '\\\\N' else None,\n",
    "                        'primary_profession': escape_sql(row.get('primary_profession', ''))\n",
    "                    }\n",
    "    \n",
    "    # Load ratings\n",
    "    ratings_file = data_path / 'ratings.tsv'\n",
    "    if ratings_file.exists():\n",
    "        with open(ratings_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                tconst = row.get('title_id', '')\n",
    "                if tconst:\n",
    "                    new_tconst = f\"NEW{tconst}\"\n",
    "                    if new_tconst in result['titles']:\n",
    "                        result['ratings'][new_tconst] = {\n",
    "                            'average_rating': row.get('average_rating', '0'),\n",
    "                            'num_votes': row.get('num_votes', '0')\n",
    "                        }\n",
    "    \n",
    "    # Load principals\n",
    "    principals_file = data_path / 'principals.tsv'\n",
    "    if principals_file.exists():\n",
    "        with open(principals_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                original_title_id = row.get('title_id', '')\n",
    "                original_person_id = row.get('person_id', '')\n",
    "                new_title_id = f\"NEW{original_title_id}\"\n",
    "                new_person_id = f\"NEW{original_person_id}\"\n",
    "                if new_title_id in result['titles'] and new_person_id in result['people']:\n",
    "                    new_key = f\"{new_title_id}_{row.get('ordering', '')}\"\n",
    "                    result['principals'][new_key] = {\n",
    "                        'title_id': new_title_id,\n",
    "                        'ordering': row.get('ordering', '0'),\n",
    "                        'person_id': new_person_id,\n",
    "                        'category': escape_sql(row.get('category', '')),\n",
    "                        'job': escape_sql(row.get('job', '')) if row.get('job') != '\\\\N' else None,\n",
    "                        'characters': escape_sql(row.get('characters', '')) if row.get('characters') != '\\\\N' else None\n",
    "                    }\n",
    "    \n",
    "    # Load aka_titles\n",
    "    aka_file = data_path / 'aka_titles.tsv'\n",
    "    if aka_file.exists():\n",
    "        with open(aka_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                original_title_id = row.get('title_id', '')\n",
    "                new_title_id = f\"NEW{original_title_id}\"\n",
    "                if new_title_id in result['titles']:\n",
    "                    new_key = f\"{new_title_id}_{row.get('ordering', '')}\"\n",
    "                    result['aka_titles'][new_key] = {\n",
    "                        'title_id': new_title_id,\n",
    "                        'ordering': row.get('ordering', '0'),\n",
    "                        'aka_title': escape_sql(row.get('aka_title', '')),\n",
    "                        'region': escape_sql(row.get('region', '')) if row.get('region') != '\\\\N' else None,\n",
    "                        'language': escape_sql(row.get('language', '')) if row.get('language') != '\\\\N' else None,\n",
    "                        'types': escape_sql(row.get('types', '')) if row.get('types') != '\\\\N' else None,\n",
    "                        'attributes': escape_sql(row.get('attributes', '')) if row.get('attributes') != '\\\\N' else None,\n",
    "                        'is_original_title': row.get('is_original_title', '0') == '1'\n",
    "                    }\n",
    "    \n",
    "    # Load episodes\n",
    "    episodes_file = data_path / 'episodes.tsv'\n",
    "    if episodes_file.exists():\n",
    "        with open(episodes_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "\n",
    "                episode_id = row.get('episode_id', '')\n",
    "                parent_id = row.get('parent_id', '')\n",
    "                \n",
    "                new_episode_id = f\"NEW{episode_id}\"\n",
    "                new_parent_id = f\"NEW{parent_id}\"\n",
    "                if new_episode_id in result['titles'] and new_parent_id in result['titles']:\n",
    "                    result['episodes'][new_episode_id] = {\n",
    "                        'parent_id': new_parent_id,\n",
    "                        'season_number': row.get('season_number', '') if row.get('season_number') != '\\\\N' else None,\n",
    "                        'episode_number': row.get('episode_number', '') if row.get('episode_number') != '\\\\N' else None\n",
    "                    }\n",
    "    \n",
    "    # Load title_genres\n",
    "    genres_file = data_path / 'title_genres.tsv'\n",
    "    if genres_file.exists():\n",
    "        with open(genres_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "            \n",
    "                title_id = row.get('title_id', '')\n",
    "                genre_value = row.get('genre', '')\n",
    "                \n",
    "                new_title_id = f\"NEW{title_id}\"\n",
    "                \n",
    "                if title_id and genre_value and new_title_id in result['titles']:\n",
    "                        new_key = f\"{new_title_id}_{genre_value}\"\n",
    "                        result['title_genres'][new_key] = {\n",
    "                            'title_id': new_title_id,\n",
    "                            'genre': genre_value\n",
    "                        }    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b871e5ae-ed75-4643-9e4a-844c9d406b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titles -> 100000\n",
      "people -> 100000\n",
      "ratings -> 77464\n",
      "principals -> 14288\n",
      "aka_titles -> 100000\n",
      "episodes -> 443\n",
      "title_genres -> 100000\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/processed\"  # względnie do katalogu projektu\n",
    "test_data = load_insert_data_from_tsv(data_dir, max_rows=100000)\n",
    "\n",
    "for table, rows in test_data.items():\n",
    "    print(table, \"->\", len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a2bbab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import statistics\n",
    "from typing import List, Tuple, Callable\n",
    "\n",
    "class BenchmarkTime:    \n",
    "    def __init__(self, db_type: str, data_dir: str):\n",
    "        self.data_dir = data_dir\n",
    "        self.db_type = db_type\n",
    "        self.process = psutil.Process()\n",
    "        self.results = []\n",
    "    \n",
    "    def get_results_df(self):\n",
    "        import pandas as pd\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "    def run_benchmark(self, scenarios: List[Tuple[str, List[Tuple[str, Callable]]]], \n",
    "                    setup_method: Callable = None, \n",
    "                    cleanup_method: Callable = None):\n",
    "        io_counters_start = psutil.disk_io_counters()\n",
    "        \n",
    "        if setup_method:\n",
    "            setup_method()\n",
    "            \n",
    "        for scenario_name, operations in scenarios:\n",
    "            start_time = time.time()\n",
    "            cpu_samples = []\n",
    "            memory_samples = []\n",
    "            durations = []\n",
    "            \n",
    "            for op_name, func in operations:\n",
    "                cpu_samples.append(self.process.cpu_percent())\n",
    "                memory_samples.append(self.process.memory_info().rss)\n",
    "                \n",
    "                op_start = time.time()\n",
    "                func()\n",
    "                op_duration = time.time() - op_start\n",
    "                durations.append(op_duration)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time = end_time - start_time\n",
    "\n",
    "            avg_cpu = statistics.mean(cpu_samples) if cpu_samples else 0\n",
    "            avg_memory = statistics.mean(memory_samples) / (1024 * 1024) if memory_samples else 0\n",
    "            \n",
    "            io_counters_end = psutil.disk_io_counters()\n",
    "            read_mb = (io_counters_end.read_bytes - io_counters_start.read_bytes) / (1024 * 1024)\n",
    "            write_mb = (io_counters_end.write_bytes - io_counters_start.write_bytes) / (1024 * 1024)\n",
    "            \n",
    "            avg_op_time = statistics.mean(durations) if durations else 0\n",
    "            throughput = len(operations) / total_time if total_time > 0 else 0\n",
    "            \n",
    "            scenario_result = {\n",
    "                'database': self.db_type,\n",
    "                'data_dir': self.data_dir,\n",
    "                'scenario': scenario_name,\n",
    "                'total_time': total_time,\n",
    "                'operations': len(operations),\n",
    "                'avg_operation_time': avg_op_time,\n",
    "                'throughput': throughput,\n",
    "                'cpu_avg': avg_cpu,\n",
    "                'memory_avg': avg_memory,\n",
    "                'disk_read_mb': read_mb,\n",
    "                'disk_write_mb': write_mb\n",
    "            }\n",
    "            self.results.append(scenario_result)\n",
    "            \n",
    "            print(f\"--- {scenario_name} ({self.db_type}) ---\")\n",
    "            print(f\"Total time: {total_time:.4f} seconds\")\n",
    "            print(f\"Operations: {len(operations)}\")\n",
    "            print(f\"Avg operation time: {avg_op_time:.4f} seconds\")\n",
    "            print(f\"Throughput: {throughput:.2f} ops/sec\")\n",
    "            print(f\"CPU avg: {avg_cpu:.2f}%\")\n",
    "            print(f\"Memory avg: {avg_memory:.2f} MB\")\n",
    "            print(f\"Disk read: {read_mb:.2f} MB\")\n",
    "            print(f\"Disk write: {write_mb:.2f} MB\")\n",
    "            print()\n",
    "            \n",
    "            io_counters_start = io_counters_end\n",
    "        \n",
    "        if cleanup_method:\n",
    "            cleanup_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2efcd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERT_TITLE=\"INSERT Title ?\"\n",
    "INSERT_PERSON=\"INSERT Person ?\"\n",
    "INSERT_RATING=\"INSERT Rating ?\"\n",
    "INSERT_PRINCIPAL=\"INSERT Principal ?\"\n",
    "INSERT_AKA_TITLE=\"INSERT AKA Title ?\"\n",
    "INSERT_EPISODE=\"INSERT Episode ?\"\n",
    "INSERT_TITLE_GENRE=\"INSERT Title Genre ?\"\n",
    "\n",
    "SELECT_TITLE = \"SELECT title ?\"\n",
    "SELECT_PERSON = \"SELECT person ?\"\n",
    "SELECT_ALL_PEOPLE_IN_TITLE = \"SELECT all people that are in the title ?\"\n",
    "SELECT_ALL_EPISODES_FOR_SERIES = \"SELECT all episodes for the series ?\"\n",
    "SELECT_ALL_RATINGS_WITH_TITLE_INFO = \"SELECT all ratings with title info for all titles in the genre ?\"\n",
    "\n",
    "UPDATE_TITLE_PRIMARY_TITLE = \"UPDATE Title ? Primary Title\"\n",
    "UPDATE_ALL_RATINGS_FOR_TITLE = \"UPDATE all Ratings for Title ?\"\n",
    "UPDATE_PERSON_PRIMARY_NAME = \"UPDATE Person ? Primary Name\"\n",
    "UPDATE_TITLE_START_YEAR = \"UPDATE Title ? Start Year\"\n",
    "UPDATE_PERSON_BIRTH_YEAR_FOR_ALL_PEOPLE_IN_TITLE = \"UPDATE Person Birth Year for all people that are in the title ?\"\n",
    "\n",
    "DELETE_TITLE = \"DELETE Title ?\"\n",
    "DELETE_PERSON = \"DELETE Person ?\"\n",
    "DELETE_GENRES_THAT_ARE_IN_THE_TITLE = \"DELETE Genres that are in the title ?\"\n",
    "DELETE_PEOPLE_WHO_ARE_IN_TITLE = \"DELETE People who are in title ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1c5c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_postgres_data(conn, data_dir: str):\n",
    "    data_path = Path(data_dir)\n",
    "\n",
    "    def copy_from_tsv(conn, table_name, file_path, columns=None):\n",
    "        op_time = time.time()\n",
    "        f_op_time = time.time()\n",
    "\n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    next(f)\n",
    "                    f_op_time = time.time()\n",
    "\n",
    "                    if columns:\n",
    "                        cur.copy_from(f,\n",
    "                            table_name,\n",
    "                            columns=columns,\n",
    "                            null=\"\\\\N\",\n",
    "                            sep=\"\\t\",\n",
    "                        )\n",
    "                    else:\n",
    "                        cur.copy_from(\n",
    "                            f,\n",
    "                            table_name,\n",
    "                            null=\"\\\\N\",\n",
    "                            sep=\"\\t\",\n",
    "                        )\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            print(f\"ERROR loading {table_name} from {file_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "        end_time = time.time()\n",
    "        return op_time, f_op_time, end_time\n",
    "\n",
    "    # ---------- T I T L E S ----------\n",
    "    titles_file = data_path / \"titles.tsv\"\n",
    "    if titles_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn,\n",
    "            \"titles\",\n",
    "            titles_file,\n",
    "            columns=(\n",
    "                \"tconst\",\n",
    "                \"title_type\",\n",
    "                \"primary_title\",\n",
    "                \"original_title\",\n",
    "                \"is_adult\",\n",
    "                \"start_year\",\n",
    "                \"end_year\",\n",
    "                \"runtime_minutes\",\n",
    "            ),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted titles in {end_time - op_time:.2f} s \"\n",
    "            f\"(COPY phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {titles_file} not found, skipping titles\")\n",
    "\n",
    "    # ---------- P E O P L E ----------\n",
    "    people_file = data_path / \"people.tsv\"\n",
    "    if people_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn,\n",
    "            \"people\",\n",
    "            people_file,\n",
    "            columns=(\n",
    "                \"nconst\",\n",
    "                \"primary_name\",\n",
    "                \"birth_year\",\n",
    "                \"death_year\",\n",
    "                \"primary_profession\",\n",
    "            ),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted people in {end_time - op_time:.2f} s \"\n",
    "            f\"(COPY phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {people_file} not found, skipping people\")\n",
    "\n",
    "    # ---------- R A T I N G S ----------\n",
    "    ratings_file = data_path / \"ratings.tsv\"\n",
    "    if ratings_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn,\n",
    "            \"ratings\",\n",
    "            ratings_file,\n",
    "            columns=(\"title_id\", \"average_rating\", \"num_votes\"),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted ratings in {end_time - op_time:.2f} s \"\n",
    "            f\"(COPY phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {ratings_file} not found, skipping ratings\")\n",
    "\n",
    "    # ---------- P R I N C I P A L S ----------\n",
    "    principals_file = data_path / \"principals.tsv\"\n",
    "    if principals_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn,\n",
    "            \"principals\",\n",
    "            principals_file,\n",
    "            columns=(\"title_id\", \"ordering\", \"person_id\", \"category\", \"job\", \"characters\"),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted principals in {end_time - op_time:.2f} s \"\n",
    "            f\"(COPY phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {principals_file} not found, skipping principals\")\n",
    "\n",
    "    # ---------- A K A _ T I T L E S ----------\n",
    "    aka_titles_file = data_path / \"aka_titles.tsv\"\n",
    "    if aka_titles_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn,\n",
    "            \"aka_titles\",\n",
    "            aka_titles_file,\n",
    "            columns=(\n",
    "                \"title_id\",\n",
    "                \"ordering\",\n",
    "                \"aka_title\",\n",
    "                \"region\",\n",
    "                \"language\",\n",
    "                \"types\",\n",
    "                \"attributes\",\n",
    "                \"is_original_title\",\n",
    "            ),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted aka_titles in {end_time - op_time:.2f} s \"\n",
    "            f\"(COPY phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {aka_titles_file} not found, skipping aka_titles\")\n",
    "\n",
    "    # ---------- E P I S O D E S ----------\n",
    "    episodes_file = data_path / \"episodes.tsv\"\n",
    "    if episodes_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn,\n",
    "            \"episodes\",\n",
    "            episodes_file,\n",
    "            columns=(\"episode_id\", \"parent_id\", \"season_number\", \"episode_number\"),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted episodes in {end_time - op_time:.2f} s \"\n",
    "            f\"(COPY phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {episodes_file} not found, skipping episodes\")\n",
    "\n",
    "    # ---------- T I T L E _ G E N R E S ----------\n",
    "    title_genres_file = data_path / \"title_genres.tsv\"\n",
    "    if title_genres_file.exists():\n",
    "        op_time, f_op_time, end_time = copy_from_tsv(\n",
    "            conn,\n",
    "            \"title_genres\",\n",
    "            title_genres_file,\n",
    "            columns=(\"title_id\", \"genre\"),\n",
    "        )\n",
    "        print(\n",
    "            f\"INFO: Inserted title_genres in {end_time - op_time:.2f} s \"\n",
    "            f\"(COPY phase: {end_time - f_op_time:.2f} s)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {title_genres_file} not found, skipping title_genres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d76a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_benchmark(data_dir, random_numbers_list) -> pd.DataFrame:\n",
    "    benchmark = BenchmarkTime(\"postgres\", data_dir)\n",
    "    def setup_for_insert():\n",
    "        with open('db/postgres/schema.sql', 'r') as f:\n",
    "            sql_schema = f.read()\n",
    "\n",
    "        initialize_postgres_schema(postgres_client, sql_schema)\n",
    "        load_postgres_data(postgres_client, data_dir)\n",
    "\n",
    "    test_data = load_insert_data_from_tsv(data_dir, max_rows=100000)\n",
    "    test_titles = test_data['titles']\n",
    "    test_people = test_data['people']\n",
    "    test_ratings = test_data['ratings']\n",
    "    test_principals = test_data['principals']\n",
    "    test_aka_titles = test_data['aka_titles']\n",
    "    test_episodes = test_data['episodes']\n",
    "    test_title_genres = test_data['title_genres']\n",
    "    \n",
    "    insert_scenarios = [\n",
    "        (\n",
    "            INSERT_TITLE,\n",
    "            [\n",
    "                (INSERT_TITLE + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO titles (tconst, title_type, primary_title, original_title, is_adult, start_year, end_year, runtime_minutes) VALUES ('{k}', '{v['title_type']}', '{v['primary_title']}', '{v['original_title']}', {v['is_adult']}, \" +\n",
    "                    (f\"{v['start_year']}\" if v.get('start_year') is not None else 'NULL') + \", \" +\n",
    "                    (f\"{v['end_year']}\" if v.get('end_year') is not None else 'NULL') + \", \" +\n",
    "                    (f\"{v['runtime_minutes']}\" if v.get('runtime_minutes') is not None else 'NULL') +\n",
    "                    \")\"))\n",
    "                for k, v in test_titles.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_PERSON,\n",
    "            [\n",
    "                (INSERT_PERSON + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO people (nconst, primary_name, birth_year, death_year, primary_profession) VALUES ('{k}', '{v['primary_name']}', \" +\n",
    "                    (f\"{v['birth_year']}\" if v.get('birth_year') is not None else 'NULL') + \", \" +\n",
    "                    (f\"{v['death_year']}\" if v.get('death_year') is not None else 'NULL') + \", \" +\n",
    "                    f\"'{v['primary_profession']}')\"))\n",
    "                for k, v in test_people.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_RATING,\n",
    "            [\n",
    "                (INSERT_RATING + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO ratings (title_id, average_rating, num_votes) VALUES ('{k}', {v['average_rating']}, {v['num_votes']})\"))\n",
    "                for k, v in test_ratings.items()\n",
    "            ]\n",
    "        ), \n",
    "        (\n",
    "            INSERT_PRINCIPAL,\n",
    "            [\n",
    "                (INSERT_PRINCIPAL + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO principals (title_id, ordering, person_id, category, job, characters) VALUES ('{v['title_id']}', {v['ordering']}, '{v['person_id']}', '{v['category']}', \" + (f\"'{v['job']}'\" if v.get('job') else 'NULL') + \", \" + (f\"'{v['characters']}'\" if v.get('characters') else 'NULL') + \")\"))\n",
    "                for k, v in test_principals.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_AKA_TITLE,\n",
    "            [\n",
    "                (INSERT_AKA_TITLE + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO aka_titles (title_id, ordering, aka_title, region, language, types, attributes, is_original_title) VALUES ('{v['title_id']}', {v['ordering']}, '{v['aka_title']}', \" + (f\"'{v['region']}'\" if v.get('region') else 'NULL') + \", \" + (f\"'{v['language']}'\" if v.get('language') else 'NULL') + \", \" + (f\"'{v['types']}'\" if v.get('types') else 'NULL') + \", \" + (f\"'{v['attributes']}'\" if v.get('attributes') else 'NULL') + f\", {v['is_original_title']})\"))\n",
    "                for k, v in test_aka_titles.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_EPISODE,\n",
    "            [\n",
    "                (INSERT_EPISODE + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO episodes (episode_id, parent_id, season_number, episode_number) VALUES ('{k}', '{v['parent_id']}', \" +\n",
    "                    (f\"{v['season_number']}\" if v.get('season_number') is not None else 'NULL') + \", \" +\n",
    "                    (f\"{v['episode_number']}\" if v.get('episode_number') is not None else 'NULL') +\n",
    "                    \")\"))\n",
    "                for k, v in test_episodes.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_TITLE_GENRE,\n",
    "            [\n",
    "                (INSERT_TITLE_GENRE + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO title_genres (title_id, genre) VALUES ('{v['title_id']}', '{v['genre']}')\"))\n",
    "                for k, v in test_title_genres.items()\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    select_scenarios = [\n",
    "        (\n",
    "            SELECT_TITLE, [\n",
    "                (SELECT_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM titles WHERE tconst = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_PERSON, [\n",
    "                (SELECT_PERSON + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM people WHERE nconst = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_PEOPLE_IN_TITLE, [\n",
    "                (SELECT_ALL_PEOPLE_IN_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM people p JOIN principals pr ON p.nconst = pr.person_id WHERE pr.title_id = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_EPISODES_FOR_SERIES, [\n",
    "                (SELECT_ALL_EPISODES_FOR_SERIES + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM episodes e JOIN titles t ON e.episode_id = t.tconst WHERE e.parent_id = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ] \n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_RATINGS_WITH_TITLE_INFO, [\n",
    "                (SELECT_ALL_RATINGS_WITH_TITLE_INFO + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM ratings r JOIN titles t ON r.title_id = t.tconst JOIN title_genres tg ON t.tconst = tg.title_id WHERE tg.genre = '{k}'\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ] \n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    update_scenarios = [\n",
    "        (\n",
    "            UPDATE_TITLE_PRIMARY_TITLE, [\n",
    "                (UPDATE_TITLE_PRIMARY_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE titles SET primary_title = 'UPDATED' WHERE tconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_ALL_RATINGS_FOR_TITLE, [\n",
    "                (UPDATE_ALL_RATINGS_FOR_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE ratings SET average_rating = 10.0 WHERE title_id = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_PERSON_PRIMARY_NAME, [\n",
    "                (UPDATE_PERSON_PRIMARY_NAME + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE people SET primary_name = 'UPDATED' WHERE nconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_TITLE_START_YEAR, [\n",
    "                (UPDATE_TITLE_START_YEAR + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE titles SET start_year = 2024 WHERE tconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_PERSON_BIRTH_YEAR_FOR_ALL_PEOPLE_IN_TITLE, [\n",
    "                (UPDATE_PERSON_BIRTH_YEAR_FOR_ALL_PEOPLE_IN_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE people SET birth_year = 2000 WHERE nconst IN (SELECT person_id FROM principals WHERE title_id = '{k}')\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    delete_scenarios = [\n",
    "        (\n",
    "            DELETE_TITLE, [\n",
    "                (DELETE_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM titles WHERE tconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_PERSON, [\n",
    "                (DELETE_PERSON + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM people WHERE nconst = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_GENRES_THAT_ARE_IN_THE_TITLE, [\n",
    "                (DELETE_GENRES_THAT_ARE_IN_THE_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM title_genres WHERE title_id = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_PEOPLE_WHO_ARE_IN_TITLE, [\n",
    "                (DELETE_PEOPLE_WHO_ARE_IN_TITLE + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM principals WHERE title_id = '{k}'\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    benchmark.run_benchmark(insert_scenarios, setup_method=setup_for_insert)\n",
    "    benchmark.run_benchmark(select_scenarios)\n",
    "    benchmark.run_benchmark(update_scenarios)\n",
    "    benchmark.run_benchmark(delete_scenarios)\n",
    "    return benchmark.get_results_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7516f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG=False\n",
    "from random import sample\n",
    "def run_all_benchmarks(scale, ignore_cassandra=False):\n",
    "    data_path = './data/scale_' + str(scale)\n",
    "    data_path = './data/processed'\n",
    "\n",
    "    # generate_files(output_dir=data_path, scale=adjusted_scale)\n",
    "    rand_list = sample(range(1, scale+1), 100000)\n",
    "\n",
    "    results = []\n",
    "    postgres_results_df = postgres_benchmark(data_path, rand_list)\n",
    "    results.append(postgres_results_df)\n",
    "\n",
    "    merged_df = pd.concat(results, ignore_index=True)\n",
    "    merged_df.to_csv(f\"{data_path}/merged_results.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a763c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL schema initialization complete.\n",
      "INFO: Inserted titles in 27.17 s (COPY phase: 27.17 s)\n"
     ]
    }
   ],
   "source": [
    "run_all_benchmarks(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7191900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acdf340-9e43-45e0-89a0-97f4cb7cc8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
