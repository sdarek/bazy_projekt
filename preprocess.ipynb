{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c215473-a7f7-4837-9a8a-11272aa15dc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "import re\n",
    "def preprocess_imdb(input_dir: str = \"./data\", output_dir: str = \"./data/processed\"):\n",
    "    \"\"\"\n",
    "    Przygotowuje pliki TSV zgodne z ERD:\n",
    "    - titles.tsv\n",
    "    - people.tsv\n",
    "    - ratings.tsv\n",
    "    - principals.tsv\n",
    "    - aka_titles.tsv\n",
    "    - episodes.tsv\n",
    "    - title_genres.tsv\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    max_field_size = sys.maxsize\n",
    "    while True:\n",
    "        try:\n",
    "            csv.field_size_limit(max_field_size)\n",
    "            break\n",
    "        except OverflowError:\n",
    "            max_field_size = max_field_size // 10\n",
    "    in_path = Path(input_dir)\n",
    "    out_path = Path(output_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # regex: tab + \" + tekst bez \" i tabów, aż do taba albo końca linii\n",
    "    quote_fix = re.compile(r'\\t\"([^\"\\t]*)(?=\\t|$)')\n",
    "    # tekst do pierwszego taba + tab + \\N - dla people\n",
    "    people_fix = re.compile(r'^([^\\t]*\\t)\\\\N')\n",
    "\n",
    "    def cleaned_lines(f, clean_people=False):\n",
    "        for line in f:\n",
    "            # zamiana: \\t\"tekst   ->  \\ttekst\n",
    "            line = quote_fix.sub(r'\\t\\1', line)\n",
    "            \n",
    "            if clean_people:\n",
    "                line = people_fix.sub(r'\\1JakisTyp', line)\n",
    "                \n",
    "            yield line\n",
    "\n",
    "    valid_tconsts = set()\n",
    "    # ---------- T I T L E S  +  T I T L E _ G E N R E S ----------\n",
    "    basics_file = in_path / \"title.basics.tsv\"\n",
    "    if basics_file.exists():\n",
    "        titles_out = out_path / \"titles.tsv\"\n",
    "        genres_out = out_path / \"title_genres.tsv\"\n",
    "\n",
    "        with basics_file.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "             titles_out.open(\"w\", encoding=\"utf-8\", newline=\"\") as ftitles, \\\n",
    "             genres_out.open(\"w\", encoding=\"utf-8\", newline=\"\") as fgenres:\n",
    "                 \n",
    "\n",
    "            reader = csv.DictReader(cleaned_lines(fin), delimiter=\"\\t\")\n",
    "            wtitles = csv.writer(ftitles, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "            wgenres = csv.writer(fgenres, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "                 \n",
    "\n",
    "            # nagłówki zgodne ze schemą\n",
    "            wtitles.writerow([\n",
    "                \"tconst\",\n",
    "                \"title_type\",\n",
    "                \"primary_title\",\n",
    "                \"original_title\",\n",
    "                \"is_adult\",\n",
    "                \"start_year\",\n",
    "                \"end_year\",\n",
    "                \"runtime_minutes\",\n",
    "            ])\n",
    "            wgenres.writerow([\"title_id\", \"genre\"])\n",
    "\n",
    "            for row in reader:\n",
    "                tconst = row.get(\"tconst\", \"\\\\N\")\n",
    "                \n",
    "                if tconst and tconst != \"\\\\N\":\n",
    "                    valid_tconsts.add(tconst) \n",
    "\n",
    "                wtitles.writerow([\n",
    "                    tconst,\n",
    "                    row.get(\"titleType\", \"\\\\N\"),\n",
    "                    row.get(\"primaryTitle\", \"\\\\N\"),\n",
    "                    row.get(\"originalTitle\", \"\\\\N\"),\n",
    "                    row.get(\"isAdult\", \"\\\\N\"),\n",
    "                    row.get(\"startYear\", \"\\\\N\"),\n",
    "                    row.get(\"endYear\", \"\\\\N\"),\n",
    "                    row.get(\"runtimeMinutes\", \"\\\\N\"),\n",
    "                ])\n",
    "\n",
    "                genres = row.get(\"genres\", \"\\\\N\")\n",
    "                if genres and genres != \"\\\\N\":\n",
    "                    for g in genres.split(\",\"):\n",
    "                        g = g.strip()\n",
    "                        if g:\n",
    "                            wgenres.writerow([tconst, g])\n",
    "        print(\"[preprocess] titles + title_genres OK\")\n",
    "    else:\n",
    "        print(\"[preprocess] WARNING: title.basics.tsv not found\")\n",
    "\n",
    "    # ---------- P E O P L E ----------\n",
    "    name_file = in_path / \"name.basics.tsv\"\n",
    "    valid_nconsts = set()\n",
    "    if name_file.exists():\n",
    "        people_out = out_path / \"people.tsv\"\n",
    "        with name_file.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "             people_out.open(\"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "\n",
    "            reader = csv.DictReader(cleaned_lines(fin, clean_people=True), delimiter=\"\\t\")\n",
    "            w = csv.writer(fout, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "\n",
    "            # nconst, primary_name, birth_year, death_year, primary_profession\n",
    "            w.writerow([\"nconst\", \"primary_name\", \"birth_year\", \"death_year\", \"primary_profession\"])\n",
    "\n",
    "            for row in reader:\n",
    "                nconst = row.get(\"nconst\", \"\\\\N\")\n",
    "                if nconst and nconst != \"\\\\N\":\n",
    "                    valid_nconsts.add(nconst)\n",
    "                w.writerow([\n",
    "                    nconst,\n",
    "                    row.get(\"primaryName\", \"\\\\N\"),\n",
    "                    row.get(\"birthYear\", \"\\\\N\"),\n",
    "                    row.get(\"deathYear\", \"\\\\N\"),\n",
    "                    row.get(\"primaryProfession\", \"\\\\N\"),\n",
    "                ])\n",
    "        print(\"[preprocess] people OK\")\n",
    "    else:\n",
    "        print(\"[preprocess] WARNING: name.basics.tsv not found\")\n",
    "\n",
    "    # ---------- R A T I N G S ----------\n",
    "    ratings_file = in_path / \"title.ratings.tsv\"\n",
    "    if ratings_file.exists():\n",
    "        ratings_out = out_path / \"ratings.tsv\"\n",
    "        with ratings_file.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "             ratings_out.open(\"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "\n",
    "            reader = csv.DictReader(fin, delimiter=\"\\t\")\n",
    "            w = csv.writer(fout, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "\n",
    "            # tconst -> title_id\n",
    "            w.writerow([\"title_id\", \"average_rating\", \"num_votes\"])\n",
    "\n",
    "            for row in reader:\n",
    "                title_id = row.get(\"tconst\", \"\\\\N\")\n",
    "                if title_id in valid_tconsts:\n",
    "                    w.writerow([\n",
    "                        title_id,\n",
    "                        row.get(\"averageRating\", \"\\\\N\"),\n",
    "                        row.get(\"numVotes\", \"\\\\N\"),\n",
    "                    ])\n",
    "        print(\"[preprocess] ratings OK\")\n",
    "    else:\n",
    "        print(\"[preprocess] WARNING: title.ratings.tsv not found\")\n",
    "\n",
    "    # ---------- P R I N C I P A L S ----------\n",
    "    MAX_PRINCIPALS = 15000000  # Limit 15mln\n",
    "    count = 0\n",
    "    principals_file = in_path / \"title.principals.tsv\"\n",
    "    if principals_file.exists():\n",
    "        principals_out = out_path / \"principals.tsv\"\n",
    "        with principals_file.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "             principals_out.open(\"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "\n",
    "            reader = csv.DictReader(fin, delimiter=\"\\t\")\n",
    "            w = csv.writer(fout, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "\n",
    "            # title_id, ordering, person_id, category, job, characters\n",
    "            w.writerow([\"title_id\", \"ordering\", \"person_id\", \"category\", \"job\", \"characters\"])\n",
    "\n",
    "            for row in reader:\n",
    "                title_id = row.get(\"tconst\", \"\\\\N\")\n",
    "                person_id = row.get(\"nconst\", \"\\\\N\")\n",
    "                if person_id in valid_nconsts and title_id in valid_tconsts:\n",
    "                    if count >= MAX_PRINCIPALS:\n",
    "                        break\n",
    "                    w.writerow([\n",
    "                        title_id,\n",
    "                        row.get(\"ordering\", \"\\\\N\"),\n",
    "                        person_id,\n",
    "                        row.get(\"category\", \"\\\\N\"),\n",
    "                        row.get(\"job\", \"\\\\N\"),\n",
    "                        row.get(\"characters\", \"\\\\N\"),\n",
    "                    ])\n",
    "                    count += 1\n",
    "        print(\"[preprocess] principals OK\")\n",
    "    else:\n",
    "        print(\"[preprocess] WARNING: title.principals.tsv not found\")\n",
    "\n",
    "    # ---------- A K A _ T I T L E S ----------\n",
    "    akas_file = in_path / \"title.akas.tsv\"\n",
    "    if akas_file.exists():\n",
    "        akas_out = out_path / \"aka_titles.tsv\"\n",
    "        with akas_file.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "             akas_out.open(\"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "\n",
    "            reader = csv.DictReader(fin, delimiter=\"\\t\")\n",
    "            w = csv.writer(fout, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "\n",
    "            # title_id, ordering, aka_title, region, language, types, attributes, is_original_title\n",
    "            w.writerow([\n",
    "                \"title_id\", \"ordering\", \"aka_title\",\n",
    "                \"region\", \"language\", \"types\", \"attributes\", \"is_original_title\"\n",
    "            ])\n",
    "\n",
    "            for row in reader:\n",
    "                title_id = row.get(\"titleId\", \"\\\\N\")\n",
    "                if title_id in valid_tconsts:\n",
    "                    w.writerow([\n",
    "                        title_id,\n",
    "                        row.get(\"ordering\", \"\\\\N\"),\n",
    "                        row.get(\"title\", \"\\\\N\"),\n",
    "                        row.get(\"region\", \"\\\\N\"),\n",
    "                        row.get(\"language\", \"\\\\N\"),\n",
    "                        row.get(\"types\", \"\\\\N\"),\n",
    "                        row.get(\"attributes\", \"\\\\N\"),\n",
    "                        row.get(\"isOriginalTitle\", \"\\\\N\"),\n",
    "                    ])\n",
    "        print(\"[preprocess] aka_titles OK\")\n",
    "    else:\n",
    "        print(\"[preprocess] WARNING: title.akas.tsv not found\")\n",
    "\n",
    "    # ---------- E P I S O D E S ----------\n",
    "    episodes_file = in_path / \"title.episode.tsv\"\n",
    "    if episodes_file.exists():\n",
    "        episodes_out = out_path / \"episodes.tsv\"\n",
    "        with episodes_file.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "             episodes_out.open(\"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "\n",
    "            reader = csv.DictReader(fin, delimiter=\"\\t\")\n",
    "            w = csv.writer(fout, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "\n",
    "            # episode_id, parent_id, season_number, episode_number\n",
    "            w.writerow([\"episode_id\", \"parent_id\", \"season_number\", \"episode_number\"])\n",
    "\n",
    "            for row in reader:\n",
    "                episode_id = row.get(\"tconst\", \"\\\\N\") \n",
    "                parent_id = row.get(\"parentTconst\", \"\\\\N\") \n",
    "                if episode_id in valid_tconsts and parent_id in valid_tconsts:\n",
    "                    w.writerow([\n",
    "                        episode_id,\n",
    "                        parent_id,\n",
    "                        row.get(\"seasonNumber\", \"\\\\N\"),\n",
    "                        row.get(\"episodeNumber\", \"\\\\N\"),\n",
    "                    ])\n",
    "        print(\"[preprocess] episodes OK\")\n",
    "    else:\n",
    "        print(\"[preprocess] WARNING: title.episode.tsv not found\")\n",
    "\n",
    "    print(\"[preprocess] DONE. Pliki w:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3abe6cfb-9355-4fda-af4b-611bf844b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "\n",
    "def generate_files(input_dir: str, max_child_rows: int):\n",
    "    \"\"\"\n",
    "    Tworzy podzbiór przetworzonych plików IMDb z zachowaniem integralności referencyjnej.\n",
    "\n",
    "    - input_dir: katalog z plikami po preprocess_imdb (titles.tsv, people.tsv, itd.)\n",
    "    - max_child_rows: maksymalna liczba wierszy w KAŻDEJ tabeli dziecka:\n",
    "        ratings, aka_titles, principals, episodes, title_genres\n",
    "\n",
    "    W nowym katalogu (np. processed_1000) powstają:\n",
    "        titles.tsv, people.tsv, ratings.tsv, principals.tsv,\n",
    "        aka_titles.tsv, episodes.tsv, title_genres.tsv\n",
    "    z prefixem 'NEW' w kluczach.\n",
    "    \"\"\"\n",
    "    in_path = Path(input_dir)\n",
    "    out_path = in_path.parent / f\"processed_{max_child_rows}\"\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    KEY_PREFIX = \"NEW\"\n",
    "\n",
    "    # Zbiór użytych kluczy (oryginalnych, bez prefiksu)\n",
    "    selected_title_ids = set()\n",
    "    selected_person_ids = set()\n",
    "\n",
    "    # --- helper do kopiowania tabel-dzieci ------------------------\n",
    "    def copy_child_table(\n",
    "        filename: str,\n",
    "        key_columns_title=None,\n",
    "        key_columns_person=None,\n",
    "        limit: int = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        filename: nazwa pliku TSV\n",
    "        key_columns_title: lista nazw kolumn zawierających FK do titles\n",
    "        key_columns_person: lista nazw kolumn zawierających FK do people\n",
    "        limit: maksymalna liczba wierszy (None => bez limitu)\n",
    "        \"\"\"\n",
    "        nonlocal selected_title_ids, selected_person_ids\n",
    "\n",
    "        key_columns_title = key_columns_title or []\n",
    "        key_columns_person = key_columns_person or []\n",
    "\n",
    "        src = in_path / filename\n",
    "        if not src.exists():\n",
    "            print(f\"[generate_files] WARNING: {filename} not found, pomijam\")\n",
    "            return\n",
    "\n",
    "        dst = out_path / filename\n",
    "\n",
    "        with src.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "             dst.open(\"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "\n",
    "            reader = csv.DictReader(fin, delimiter=\"\\t\")\n",
    "            if reader.fieldnames is None:\n",
    "                print(f\"[generate_files] WARNING: {filename} ma pusty nagłówek?\")\n",
    "                return\n",
    "\n",
    "            writer = csv.writer(fout, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "            writer.writerow(reader.fieldnames)\n",
    "\n",
    "            count = 0\n",
    "            for row in reader:\n",
    "                if limit is not None and count >= limit:\n",
    "                    break\n",
    "\n",
    "                # zbieramy oryginalne ID (bez prefiksu)\n",
    "                for col in key_columns_title:\n",
    "                    val = row.get(col, \"\\\\N\")\n",
    "                    if val and val != \"\\\\N\":\n",
    "                        selected_title_ids.add(val)\n",
    "\n",
    "                for col in key_columns_person:\n",
    "                    val = row.get(col, \"\\\\N\")\n",
    "                    if val and val != \"\\\\N\":\n",
    "                        selected_person_ids.add(val)\n",
    "\n",
    "                # przygotowujemy wiersz wyjściowy z prefiksem w kolumnach-kluczach\n",
    "                out_row = []\n",
    "                for col in reader.fieldnames:\n",
    "                    val = row.get(col, \"\\\\N\")\n",
    "                    if val and val != \"\\\\N\":\n",
    "                        if col in key_columns_title or col in key_columns_person:\n",
    "                            val = KEY_PREFIX + val\n",
    "                    out_row.append(val)\n",
    "\n",
    "                writer.writerow(out_row)\n",
    "                count += 1\n",
    "\n",
    "        print(f\"[generate_files] {filename} OK (zapisano {count} wierszy)\")\n",
    "\n",
    "    # --- 1) Tabele dzieci -----------------------------------------\n",
    "    # ratings: title_id (FK -> titles)\n",
    "    copy_child_table(\n",
    "        \"ratings.tsv\",\n",
    "        key_columns_title=[\"title_id\"],\n",
    "        key_columns_person=[],\n",
    "        limit=max_child_rows,\n",
    "    )\n",
    "\n",
    "    # aka_titles: title_id (FK -> titles)\n",
    "    copy_child_table(\n",
    "        \"aka_titles.tsv\",\n",
    "        key_columns_title=[\"title_id\"],\n",
    "        key_columns_person=[],\n",
    "        limit=max_child_rows,\n",
    "    )\n",
    "\n",
    "    # title_genres: title_id (FK -> titles)\n",
    "    copy_child_table(\n",
    "        \"title_genres.tsv\",\n",
    "        key_columns_title=[\"title_id\"],\n",
    "        key_columns_person=[],\n",
    "        limit=max_child_rows,\n",
    "    )\n",
    "\n",
    "    # episodes: episode_id, parent_id (oba FK -> titles)\n",
    "    copy_child_table(\n",
    "        \"episodes.tsv\",\n",
    "        key_columns_title=[\"episode_id\", \"parent_id\"],\n",
    "        key_columns_person=[],\n",
    "        limit=max_child_rows,\n",
    "    )\n",
    "\n",
    "    # principals: title_id (FK -> titles), person_id (FK -> people)\n",
    "    copy_child_table(\n",
    "        \"principals.tsv\",\n",
    "        key_columns_title=[\"title_id\"],\n",
    "        key_columns_person=[\"person_id\"],\n",
    "        limit=max_child_rows,\n",
    "    )\n",
    "\n",
    "    # --- 2) Tabele rodziców ---------------------------------------\n",
    "\n",
    "    # titles: tconst w selected_title_ids\n",
    "    titles_src = in_path / \"titles.tsv\"\n",
    "    titles_dst = out_path / \"titles.tsv\"\n",
    "    if titles_src.exists():\n",
    "        with titles_src.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "             titles_dst.open(\"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "\n",
    "            reader = csv.DictReader(fin, delimiter=\"\\t\")\n",
    "            writer = csv.writer(fout, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "\n",
    "            writer.writerow(reader.fieldnames)\n",
    "\n",
    "            kept = 0\n",
    "            for row in reader:\n",
    "                tconst = row.get(\"tconst\", \"\\\\N\")\n",
    "                if tconst in selected_title_ids:\n",
    "                    out_row = []\n",
    "                    for col in reader.fieldnames:\n",
    "                        val = row.get(col, \"\\\\N\")\n",
    "                        if col == \"tconst\" and val and val != \"\\\\N\":\n",
    "                            val = KEY_PREFIX + val\n",
    "                        out_row.append(val)\n",
    "                    writer.writerow(out_row)\n",
    "                    kept += 1\n",
    "\n",
    "        print(f\"[generate_files] titles.tsv OK (zapisano {kept} wierszy)\")\n",
    "    else:\n",
    "        print(\"[generate_files] WARNING: titles.tsv not found\")\n",
    "\n",
    "    # people: nconst w selected_person_ids\n",
    "    people_src = in_path / \"people.tsv\"\n",
    "    people_dst = out_path / \"people.tsv\"\n",
    "    if people_src.exists():\n",
    "        with people_src.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "             people_dst.open(\"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "\n",
    "            reader = csv.DictReader(fin, delimiter=\"\\t\")\n",
    "            writer = csv.writer(fout, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "\n",
    "            writer.writerow(reader.fieldnames)\n",
    "\n",
    "            kept = 0\n",
    "            for row in reader:\n",
    "                nconst = row.get(\"nconst\", \"\\\\N\")\n",
    "                if nconst in selected_person_ids:\n",
    "                    out_row = []\n",
    "                    for col in reader.fieldnames:\n",
    "                        val = row.get(col, \"\\\\N\")\n",
    "                        if col == \"nconst\" and val and val != \"\\\\N\":\n",
    "                            val = KEY_PREFIX + val\n",
    "                        out_row.append(val)\n",
    "                    writer.writerow(out_row)\n",
    "                    kept += 1\n",
    "\n",
    "        print(f\"[generate_files] people.tsv OK (zapisano {kept} wierszy)\")\n",
    "    else:\n",
    "        print(\"[generate_files] WARNING: people.tsv not found\")\n",
    "\n",
    "    print(f\"[generate_files] DONE. Nowe pliki w: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde2edc1-4f01-42c7-bb89-088c7f02bcf3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preprocess] titles + title_genres OK\n",
      "[preprocess] people OK\n",
      "[preprocess] ratings OK\n",
      "[preprocess] principals OK\n",
      "[preprocess] aka_titles OK\n",
      "[preprocess] episodes OK\n",
      "[preprocess] DONE. Pliki w: data\\processed2\n"
     ]
    }
   ],
   "source": [
    "preprocess_imdb(input_dir=\"./data\", output_dir=\"./data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48ca564c-e65e-4620-a432-15b4e8412ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generate_files] ratings.tsv OK (zapisano 100 wierszy)\n",
      "[generate_files] aka_titles.tsv OK (zapisano 100 wierszy)\n",
      "[generate_files] title_genres.tsv OK (zapisano 100 wierszy)\n",
      "[generate_files] episodes.tsv OK (zapisano 100 wierszy)\n",
      "[generate_files] principals.tsv OK (zapisano 100 wierszy)\n",
      "[generate_files] titles.tsv OK (zapisano 234 wierszy)\n",
      "[generate_files] people.tsv OK (zapisano 39 wierszy)\n",
      "[generate_files] DONE. Nowe pliki w: data\\processed_100\n"
     ]
    }
   ],
   "source": [
    "generate_files(input_dir=\"./data/processed\", max_child_rows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7b869a7-2155-4f39-af8f-9a2dbf51dc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generate_files] ratings.tsv OK (zapisano 1000 wierszy)\n",
      "[generate_files] aka_titles.tsv OK (zapisano 1000 wierszy)\n",
      "[generate_files] title_genres.tsv OK (zapisano 1000 wierszy)\n",
      "[generate_files] episodes.tsv OK (zapisano 1000 wierszy)\n",
      "[generate_files] principals.tsv OK (zapisano 1000 wierszy)\n",
      "[generate_files] titles.tsv OK (zapisano 2387 wierszy)\n",
      "[generate_files] people.tsv OK (zapisano 243 wierszy)\n",
      "[generate_files] DONE. Nowe pliki w: data\\processed_1000\n"
     ]
    }
   ],
   "source": [
    "generate_files(input_dir=\"./data/processed\", max_child_rows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "316c4a8f-7f61-4f8e-9358-87d5ed466c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generate_files] ratings.tsv OK (zapisano 10000 wierszy)\n",
      "[generate_files] aka_titles.tsv OK (zapisano 10000 wierszy)\n",
      "[generate_files] title_genres.tsv OK (zapisano 10000 wierszy)\n",
      "[generate_files] episodes.tsv OK (zapisano 10000 wierszy)\n",
      "[generate_files] principals.tsv OK (zapisano 10000 wierszy)\n",
      "[generate_files] titles.tsv OK (zapisano 24931 wierszy)\n",
      "[generate_files] people.tsv OK (zapisano 1663 wierszy)\n",
      "[generate_files] DONE. Nowe pliki w: data\\processed_10000\n"
     ]
    }
   ],
   "source": [
    "generate_files(input_dir=\"./data/processed\", max_child_rows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95afe036-e484-4a28-89be-29a59de56893",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_files(input_dir=\"./data/processed\", max_child_rows=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be1da1-b509-4ba5-a8b3-eed6ade8bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_files(input_dir=\"./data/processed\", max_child_rows=1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
